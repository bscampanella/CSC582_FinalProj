{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3612jvsc74a57bd01917c364b50b373201183b69312b4f370f1c66530484ad23d950c582ad587b05",
   "display_name": "Python 3.6.12 64-bit ('synqg': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk import FreqDist\n",
    "from urllib import request\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "charles_dickens = [\"https://www.gutenberg.org/files/98/98-0.txt\", \"https://www.gutenberg.org/files/46/46-0.txt\", \"https://www.gutenberg.org/files/1400/1400-0.txt\", \"https://www.gutenberg.org/files/730/730-0.txt\", \"https://www.gutenberg.org/files/766/766-0.txt\"]\n",
    "\n",
    "marry_shelly = [\"https://www.gutenberg.org/files/84/84-0.txt\", \"https://www.gutenberg.org/files/18247/18247-0.txt\", \"https://www.gutenberg.org/cache/epub/15238/pg15238.txt\", \"https://www.gutenberg.org/cache/epub/6447/pg6447.txt\", \"https://www.gutenberg.org/files/63337/63337-0.txt\", ]\n",
    "\n",
    "austin_jane = [\"https://www.gutenberg.org/files/1342/1342-0.txt\", \"https://www.gutenberg.org/files/158/158-0.txt\", \"https://www.gutenberg.org/files/161/161-0.txt\", \"https://www.gutenberg.org/cache/epub/105/pg105.txt\", \"https://www.gutenberg.org/files/121/121-0.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetRawText(url):\n",
    "    response = request.urlopen(url)\n",
    "    raw = response.read().decode('utf8')\n",
    "    #remove non book stuff\n",
    "    start_index = raw.find(\"*** START\")\n",
    "    end_of_line = raw.find(\"\\n\", start_index)\n",
    "    return raw[end_of_line : ]\n",
    "\n",
    "\n",
    "#example: get all of charles dickens work to examine statistics\n",
    "raw_str_arr = []\n",
    "for url_ in charles_dickens:\n",
    "    #raw_str_arr.append( GetRawText(url_) )\n",
    "    pass\n",
    "\n",
    "raw_cd = GetRawText(charles_dickens[0])\n",
    "with open(\"my_cd_1.txt\", \"w\") as f:\n",
    "    f.write(raw_cd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#print(raw_str_arr)\n",
    "\n",
    "all_CD_text = \" \".join(raw_str_arr)\n",
    "\n",
    "#print(raw_str)\n",
    "\n",
    "tokenized_sentences_CD = sent_tokenize(all_CD_text)\n",
    "print(len(tokenized_sentences_CD))\n",
    "print(tokenized_sentences_CD[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def avgLenSentence(list_of_sentences):\n",
    "    sent_len = []\n",
    "    for ts in list_of_sentences:\n",
    "        sent_len.append(len(word_tokenize(ts)))\n",
    "\n",
    "    return (sum(sent_len)/len(sent_len))\n",
    "\n",
    "CD_avg_sent_len = avgLenSentence(tokenized_sentences_CD)\n",
    "print(CD_avg_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try and identify most commonly used non-stopwords\n",
    "\n",
    "def most_common_words(raw_text):\n",
    "    tokenized_words = word_tokenize(raw_text)\n",
    "    cleaned_words = []\n",
    "\n",
    "    for w in tokenized_words:\n",
    "        w = w.lower()\n",
    "        if w not in stop_words and w not in \"\"\",.'\"!?;:`~--()\"\"\":\n",
    "            cleaned_words.append(w)\n",
    "    \n",
    "    fqdst = FreqDist(cleaned_words)\n",
    "\n",
    "    return fqdst\n",
    "\n",
    "CDFqdst = most_common_words(all_CD_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CDFqdst.most_common(100))\n",
    "\n",
    "f = CDFqdst[\"mr.\"]\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_ngram(raw_text, n = 2):\n",
    "    tokenized_words = word_tokenize(raw_text)\n",
    "    cleaned_words = []\n",
    "\n",
    "    for w in tokenized_words:\n",
    "        w = w.lower()\n",
    "        #if w not in stop_words:\n",
    "        cleaned_words.append(w)\n",
    "    \n",
    "\n",
    "    bigrams = nltk.ngrams(cleaned_words, n)\n",
    "    fqdst = FreqDist(bigrams)\n",
    "\n",
    "    return fqdst\n",
    "\n",
    "CDBgramFqdst = most_common_ngram(all_CD_text,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CDBgramFqdst.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}