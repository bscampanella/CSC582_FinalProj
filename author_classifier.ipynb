{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3612jvsc74a57bd01917c364b50b373201183b69312b4f370f1c66530484ad23d950c582ad587b05",
   "display_name": "Python 3.6.12 64-bit ('synqg': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package cmudict to\n[nltk_data]     C:\\Users\\bscam\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package cmudict is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\bscam\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections as coll\n",
    "import math\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import style\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "\n",
    "nltk.download('cmudict')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "style.use(\"ggplot\")\n",
    "cmuDictionary = None\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def syllable_count_Manual(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "            if word.endswith(\"e\"):\n",
    "                count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# COUNTS NUMBER OF SYLLABLES\n",
    "\n",
    "def syllable_count(word):\n",
    "    global cmuDictionary\n",
    "    d = cmuDictionary\n",
    "    try:\n",
    "        syl = [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    except:\n",
    "        syl = syllable_count_Manual(word)\n",
    "    return syl\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# removing stop words plus punctuation.\n",
    "def Avg_wordLength(str):\n",
    "    str.translate(string.punctuation)\n",
    "    tokens = word_tokenize(str, language='english')\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    stop = stopwords.words('english') + st\n",
    "    words = [word for word in tokens if word not in stop]\n",
    "    return np.average([len(word) for word in words])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# returns avg number of characters in a sentence\n",
    "def Avg_SentLenghtByCh(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token) for token in tokens])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# returns avg number of words in a sentence\n",
    "def Avg_SentLenghtByWord(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token.split()) for token in tokens])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# GIVES NUMBER OF SYLLABLES PER WORD\n",
    "def Avg_Syllable_per_Word(text):\n",
    "    tokens = word_tokenize(text, language='english')\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    stop = stopwords.words('english') + st\n",
    "    words = [word for word in tokens if word not in stop]\n",
    "    syllabls = [syllable_count(word) for word in words]\n",
    "    p = (\" \".join(words))\n",
    "    return sum(syllabls) / max(1, len(words))\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# COUNTS SPECIAL CHARACTERS NORMALIZED OVER LENGTH OF CHUNK\n",
    "def CountSpecialCharacter(text):\n",
    "    st = [\"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \"/\", \"<\", \"=\", '>',\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    count = 0\n",
    "    for i in text:\n",
    "        if (i in st):\n",
    "            count = count + 1\n",
    "    return count / len(text)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def CountPuncuation(text):\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \";\", \"?\", \":\", \";\"]\n",
    "    count = 0\n",
    "    for i in text:\n",
    "        if (i in st):\n",
    "            count = count + 1\n",
    "    return float(count) / float(len(text))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# RETURNS NORMALIZED COUNT OF FUNCTIONAL WORDS FROM A Framework for\n",
    "# Authorship Identification of Online Messages: Writing-Style Features and Classification Techniques\n",
    "\n",
    "def CountFunctionalWords(text):\n",
    "    functional_words = \"\"\"a between in nor some upon\n",
    "    about both including nothing somebody us\n",
    "    above but inside of someone used\n",
    "    after by into off something via\n",
    "    all can is on such we\n",
    "    although cos it once than what\n",
    "    am do its one that whatever\n",
    "    among down latter onto the when\n",
    "    an each less opposite their where\n",
    "    and either like or them whether\n",
    "    another enough little our these which\n",
    "    any every lots outside they while\n",
    "    anybody everybody many over this who\n",
    "    anyone everyone me own those whoever\n",
    "    anything everything more past though whom\n",
    "    are few most per through whose\n",
    "    around following much plenty till will\n",
    "    as for must plus to with\n",
    "    at from my regarding toward within\n",
    "    be have near same towards without\n",
    "    because he need several under worth\n",
    "    before her neither she unless would\n",
    "    behind him no should unlike yes\n",
    "    below i nobody since until you\n",
    "    beside if none so up your\n",
    "    \"\"\"\n",
    "\n",
    "    functional_words = functional_words.split()\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    count = 0\n",
    "\n",
    "    for i in text:\n",
    "        if i in functional_words:\n",
    "            count += 1\n",
    "\n",
    "    return count / len(words)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# also returns Honore Measure R\n",
    "#s (HonoreÂ´, 1979, quoted in Holmes and Singh, 1996)\n",
    "def hapaxLegemena(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    V1 = 0\n",
    "    # dictionary comprehension . har word kay against value 0 kardi\n",
    "    freqs = {key: 0 for key in words}\n",
    "    for word in words:\n",
    "        freqs[word] += 1\n",
    "    for word in freqs:\n",
    "        if freqs[word] == 1:\n",
    "            V1 += 1\n",
    "    N = len(words)\n",
    "    V = float(len(set(words)))\n",
    "    R = 100 * math.log(N) / max(1, (1 - (V1 / V)))\n",
    "    h = V1 / N\n",
    "    return R, h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# TYPE TOKEN RATIO NO OF DIFFERENT WORDS / NO OF WORDS\n",
    "def typeTokenRatio(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "def RemoveSpecialCHs(text):\n",
    "    text = word_tokenize(text)\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "\n",
    "    words = [word for word in text if word not in st]\n",
    "    return words\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# K  10,000 * (M - N) / N**2\n",
    "# , where M  Sigma i**2 * Vi.\n",
    "def YulesCharacteristicK(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    N = len(words)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    vi = coll.Counter()\n",
    "    vi.update(freqs.values())\n",
    "    M = sum([(value * value) * vi[value] for key, value in freqs.items()])\n",
    "    K = 10000 * (M - N) / math.pow(N, 2)\n",
    "    return K\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -1*sigma(pi*lnpi)\n",
    "# Shannon and sympsons index are basically diversity indices for any community\n",
    "def ShannonEntropy(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    lenght = len(words)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    arr = np.array(list(freqs.values()))\n",
    "    distribution = 1. * arr\n",
    "    distribution /= max(1, lenght)\n",
    "    import scipy as sc\n",
    "    H = sc.stats.entropy(distribution, base=2)\n",
    "    # H = sum([(i/lenght)*math.log(i/lenght,math.e) for i in freqs.values()])\n",
    "    return H\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1 - (sigma(n(n - 1))/N(N-1)\n",
    "# N is total number of words\n",
    "# n is the number of each type of word\n",
    "def SimpsonsIndex(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    N = len(words)\n",
    "    n = sum([1.0 * i * (i - 1) for i in freqs.values()])\n",
    "    D = 1 - (n / (N * (N - 1)))\n",
    "    return D\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def FleschReadingEase(text, NoOfsentences ):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    l = float(len(words))\n",
    "    scount = 0\n",
    "    for word in words:\n",
    "        scount += syllable_count(word)\n",
    "\n",
    "    I = 206.835 - 1.015 * (l / float(NoOfsentences)) - 84.6 * (scount / float(l))\n",
    "    return I\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "def FleschCincadeGradeLevel(text, NoOfSentences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    scount = 0\n",
    "    for word in words:\n",
    "        scount += syllable_count(word)\n",
    "\n",
    "    l = len(words)\n",
    "    F = 0.39 * (l / NoOfSentences) + 11.8 * (scount / float(l)) - 15.59\n",
    "    return F\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "def dale_chall_readability_formula(text, NoOfSectences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    difficult = 0\n",
    "    adjusted = 0\n",
    "    NoOfWords = len(words)\n",
    "    with open('dale-chall.pkl', 'rb') as f:\n",
    "        fimiliarWords = pickle.load(f)\n",
    "    for word in words:\n",
    "        if word not in fimiliarWords:\n",
    "            difficult += 1\n",
    "    percent = (difficult / NoOfWords) * 100\n",
    "    if (percent > 5):\n",
    "        adjusted = 3.6365\n",
    "    D = 0.1579 * (percent) + 0.0496 * (NoOfWords / NoOfSectences) + adjusted\n",
    "    return D\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "def GunningFoxIndex(text, NoOfSentences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    NoOFWords = float(len(words))\n",
    "    complexWords = 0\n",
    "    for word in words:\n",
    "        if (syllable_count(word) > 2):\n",
    "            complexWords += 1\n",
    "\n",
    "    G = 0.4 * ((NoOFWords / NoOfSentences) + 100 * (complexWords / NoOFWords))\n",
    "    return G\n",
    "\n",
    "\n",
    "def getNumSentences(text):\n",
    "    return len([s for s in sent_tokenize(text)])\n",
    "\n",
    "\n",
    "def PrepareData(text1, text2, Winsize):\n",
    "    chunks1 = slidingWindow(text1, Winsize, Winsize)\n",
    "    chunks2 = slidingWindow(text2, Winsize, Winsize)\n",
    "    return \" \".join(str(chunk1) + str(chunk2) for chunk1, chunk2 in zip(chunks1, chunks2))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# returns a feature vector of text\n",
    "def FeatureExtration(text):\n",
    "    # cmu dictionary for syllables\n",
    "    global cmuDictionary\n",
    "    cmuDictionary = cmudict.dict()\n",
    "\n",
    "    chunk = text\n",
    "    feature = []\n",
    "\n",
    "    sent_num = getNumSentences(chunk)\n",
    "\n",
    "    # LEXICAL FEATURES\n",
    "    meanwl = (Avg_wordLength(chunk))\n",
    "    feature.append(meanwl)\n",
    "    \n",
    "    meansl = (Avg_SentLenghtByCh(chunk))\n",
    "    feature.append(meansl)\n",
    "    \n",
    "    mean = (Avg_SentLenghtByWord(chunk))\n",
    "    feature.append(mean)\n",
    "    \n",
    "    meanSyllable = Avg_Syllable_per_Word(chunk)\n",
    "    feature.append(meanSyllable)\n",
    "    \n",
    "    means = CountSpecialCharacter(chunk)\n",
    "    feature.append(means)\n",
    "    \n",
    "    p = CountPuncuation(chunk)\n",
    "    feature.append(p)\n",
    "    \n",
    "    f = CountFunctionalWords(text)\n",
    "    feature.append(f)\n",
    "    \n",
    "    print(\"1/2 feature\")\n",
    "    # VOCABULARY RICHNESS FEATURES\n",
    "    \n",
    "    TTratio = typeTokenRatio(chunk)\n",
    "    feature.append(TTratio)\n",
    "    \n",
    "    HonoreMeasureR, hapax = hapaxLegemena(chunk)\n",
    "    feature.append(hapax)\n",
    "    feature.append(HonoreMeasureR)\n",
    "    \n",
    "    YuleK = YulesCharacteristicK(chunk)\n",
    "    feature.append(YuleK)\n",
    "    \n",
    "    S = SimpsonsIndex(chunk)\n",
    "    feature.append(S)\n",
    "    \n",
    "    Shannon = ShannonEntropy(text)\n",
    "    feature.append(Shannon)\n",
    "\n",
    "    # READIBILTY FEATURES\n",
    "    FR = FleschReadingEase(chunk, sent_num)\n",
    "    feature.append(FR)\n",
    "\n",
    "    FC = FleschCincadeGradeLevel(chunk, sent_num)\n",
    "    feature.append(FC)\n",
    "\n",
    "    # also quite a different\n",
    "    D = dale_chall_readability_formula(chunk, sent_num)\n",
    "    feature.append(D)\n",
    "\n",
    "    # quite a difference\n",
    "    G = GunningFoxIndex(chunk, sent_num)\n",
    "    feature.append(G)\n",
    "    \n",
    "    print(\"finish feature\")\n",
    "    #vector.append(feature)\n",
    "\n",
    "    return feature\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     # You can try any text file here\n",
    "#     text = open(\"my_cd_1.txt\").read()\n",
    "\n",
    "#     vector = FeatureExtration(text)\n",
    "#     print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk import FreqDist\n",
    "from urllib import request\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "\n",
    "charles_dickens = [\"https://www.gutenberg.org/files/98/98-0.txt\", \"https://www.gutenberg.org/files/46/46-0.txt\", \"https://www.gutenberg.org/files/1400/1400-0.txt\", \"https://www.gutenberg.org/files/730/730-0.txt\", \"https://www.gutenberg.org/files/766/766-0.txt\", \"https://www.gutenberg.org/cache/epub/25985/pg25985.txt\", \"https://www.gutenberg.org/files/676/676-0.txt\", \"https://www.gutenberg.org/cache/epub/1023/pg1023.txt\", \"https://www.gutenberg.org/cache/epub/37121/pg37121.txt\", \"https://www.gutenberg.org/files/42232/42232-0.txt\", \"https://www.gutenberg.org/cache/epub/41894/pg41894.txt\", \"https://www.gutenberg.org/cache/epub/1415/pg1415.txt\", \"https://www.gutenberg.org/cache/epub/1394/pg1394.txt\"]\n",
    "\n",
    "marry_shelly = [\"https://www.gutenberg.org/files/84/84-0.txt\", \"https://www.gutenberg.org/files/18247/18247-0.txt\", \"https://www.gutenberg.org/cache/epub/15238/pg15238.txt\", \"https://www.gutenberg.org/cache/epub/6447/pg6447.txt\", \"https://www.gutenberg.org/files/56665/56665-0.txt\", \"https://www.gutenberg.org/files/63337/63337-0.txt\", \"https://www.gutenberg.org/files/63338/63338-0.txt\", \"https://www.gutenberg.org/files/63339/63339-0.txt\", \"https://www.gutenberg.org/files/64555/64555-0.txt\", \"https://www.gutenberg.org/files/64556/64556-0.txt\", \"https://www.gutenberg.org/files/64557/64557-0.txt\", \"https://www.gutenberg.org/cache/epub/4695/pg4695.txt\"]\n",
    "\n",
    "austin_jane = [\"https://www.gutenberg.org/files/1342/1342-0.txt\", \"https://www.gutenberg.org/files/158/158-0.txt\", \"https://www.gutenberg.org/files/161/161-0.txt\", \"https://www.gutenberg.org/cache/epub/105/pg105.txt\", \"https://www.gutenberg.org/files/121/121-0.txt\", \"https://www.gutenberg.org/files/63569/63569-0.txt\", \"https://www.gutenberg.org/files/141/141-0.txt\", \"https://www.gutenberg.org/cache/epub/946/pg946.txt\", \"https://www.gutenberg.org/cache/epub/42078/pg42078.txt\", \"https://www.gutenberg.org/files/1212/1212-0.txt\"]\n",
    "\n",
    "\n",
    "mark_twain = [\"https://www.gutenberg.org/files/142/142-0.txt\", \"https://www.gutenberg.org/files/76/76-0.txt\", \"https://www.gutenberg.org/files/76/76-0.txt\", \"https://www.gutenberg.org/files/3184/3184-0.txt\", \"https://www.gutenberg.org/files/3179/3179-0.txt\", \"https://www.gutenberg.org/cache/epub/19987/pg19987.txt\",\n",
    "\"https://www.gutenberg.org/files/3187/3187-0.txt\", \"https://www.gutenberg.org/files/86/86-0.txt\", \"https://www.gutenberg.org/files/3192/3192-0.txt\", \"https://www.gutenberg.org/files/3180/3180-0.txt\", \"https://www.gutenberg.org/files/3178/3178-0.txt\", \"https://www.gutenberg.org/files/3176/3176-0.txt\"]\n",
    "\n",
    "hg_wells = [\"https://www.gutenberg.org/files/59774/59774-0.txt\", \"https://www.gutenberg.org/files/524/524-0.txt\", \"https://www.gutenberg.org/cache/epub/19229/pg19229.txt\", \"https://www.gutenberg.org/files/59769/59769-0.txt\", \"https://www.gutenberg.org/files/1013/1013-0.txt\", \"https://www.gutenberg.org/files/456/456-0.txt\", \"https://www.gutenberg.org/cache/epub/11502/pg11502.txt\", \"https://www.gutenberg.org/cache/epub/3690/pg3690.txt\", \"https://www.gutenberg.org/files/1046/1046-0.txt\", \"https://www.gutenberg.org/files/3797/3797-0.txt\", \"https://www.gutenberg.org/files/5230/5230-0.txt\", \"https://www.gutenberg.org/cache/epub/159/pg159.txt\", \"https://www.gutenberg.org/cache/epub/39162/pg39162.txt\", \"https://www.gutenberg.org/cache/epub/11640/pg11640.txt\", \"https://www.gutenberg.org/files/1047/1047-0.txt\", \"https://www.gutenberg.org/files/60173/60173-0.txt\"]\n",
    "\n",
    "\n",
    "link_to_authors = [charles_dickens, marry_shelly, austin_jane, mark_twain, hg_wells]\n",
    "\n",
    "link_to_authors = link_to_authors[:2] #if i want to make it small for testing purposes\n",
    "\n",
    "def GetRawText(url):\n",
    "    response = request.urlopen(url)\n",
    "    raw = response.read().decode('utf8', \"ignore\")\n",
    "    #remove non book stuff\n",
    "    start_index = raw.find(\"***\")\n",
    "    end_of_line = raw.find(\"\\n\", start_index)\n",
    "    return raw[end_of_line : ]\n",
    "\n",
    "\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "testX = []\n",
    "testY = []\n",
    "\n",
    "\n",
    "testX_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['https://www.gutenberg.org/files/98/98-0.txt', 'https://www.gutenberg.org/files/46/46-0.txt', 'https://www.gutenberg.org/files/1400/1400-0.txt', 'https://www.gutenberg.org/files/730/730-0.txt', 'https://www.gutenberg.org/files/766/766-0.txt', 'https://www.gutenberg.org/cache/epub/25985/pg25985.txt', 'https://www.gutenberg.org/files/676/676-0.txt', 'https://www.gutenberg.org/cache/epub/1023/pg1023.txt', 'https://www.gutenberg.org/cache/epub/37121/pg37121.txt', 'https://www.gutenberg.org/files/42232/42232-0.txt', 'https://www.gutenberg.org/cache/epub/41894/pg41894.txt', 'https://www.gutenberg.org/cache/epub/1415/pg1415.txt', 'https://www.gutenberg.org/cache/epub/1394/pg1394.txt']\n",
      "https://www.gutenberg.org/files/1400/1400-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/37121/pg37121.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/676/676-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/730/730-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/1394/pg1394.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/766/766-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/42232/42232-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/1023/pg1023.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/25985/pg25985.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/46/46-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/98/98-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/41894/pg41894.txt\n",
      "in test append\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/1415/pg1415.txt\n",
      "in test append\n",
      "1/2 feature\n",
      "finish feature\n",
      "['https://www.gutenberg.org/files/84/84-0.txt', 'https://www.gutenberg.org/files/18247/18247-0.txt', 'https://www.gutenberg.org/cache/epub/15238/pg15238.txt', 'https://www.gutenberg.org/cache/epub/6447/pg6447.txt', 'https://www.gutenberg.org/files/56665/56665-0.txt', 'https://www.gutenberg.org/files/63337/63337-0.txt', 'https://www.gutenberg.org/files/63338/63338-0.txt', 'https://www.gutenberg.org/files/63339/63339-0.txt', 'https://www.gutenberg.org/files/64555/64555-0.txt', 'https://www.gutenberg.org/files/64556/64556-0.txt', 'https://www.gutenberg.org/files/64557/64557-0.txt', 'https://www.gutenberg.org/cache/epub/4695/pg4695.txt']\n",
      "https://www.gutenberg.org/cache/epub/4695/pg4695.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/15238/pg15238.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/63338/63338-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/84/84-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/63337/63337-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/64556/64556-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/64555/64555-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/63339/63339-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/6447/pg6447.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/18247/18247-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/64557/64557-0.txt\n",
      "in test append\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/56665/56665-0.txt\n",
      "in test append\n",
      "1/2 feature\n",
      "finish feature\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#print(GetRawText(\"https://www.gutenberg.org/files/676/676-0.txt\"))\n",
    "#print(FeatureExtration(GetRawText(\"https://www.gutenberg.org/files/676/676-0.txt\")))\n",
    "\n",
    "\n",
    "\n",
    "for a, author in enumerate(link_to_authors):\n",
    "    print(author)\n",
    "    randomindex = [x for x in range(len(author))]\n",
    "    random.shuffle(randomindex) \n",
    "    author = [author[ri] for ri in randomindex]\n",
    "\n",
    "    for l, link in enumerate(author):\n",
    "        print(link)\n",
    "        if l < len(author) - 2:\n",
    "            trainY.append(a)\n",
    "            trainX.append(FeatureExtration(GetRawText(link)))\n",
    "        else:\n",
    "            print(\"in test append\")\n",
    "            testY.append(a)\n",
    "            testX_text.append(GetRawText(link))\n",
    "            testX.append(FeatureExtration(testX_text[-1]))\n",
    "\n",
    "\n",
    "randomindex = [x for x in range(len(trainX))]\n",
    "random.shuffle(randomindex)\n",
    "\n",
    "trainX = [trainX[ri] for ri in randomindex]\n",
    "trainY = [trainY[ri] for ri in randomindex]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "21\n4\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX))\n",
    "print(len(testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run this cell if you wand to save the training and testing data dimensions\n",
    "all_data = [testX, testY, trainX, trainY, testX_text]\n",
    "\n",
    "with open(\"bookdata.json\", \"w\") as f:\n",
    "    json.dump(all_data,f, indent=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#only run this if you want to load previously saved data\n",
    "with open(\"bookdata.json\", \"r\") as f:\n",
    "    all_data = json.load(f)\n",
    "#print(all_data)\n",
    "\n",
    "testX  = all_data[0]\n",
    "testY = all_data[1]\n",
    "trainX = all_data[2]\n",
    "trainY = all_data[3]\n",
    "testX_text = all_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 0 1 1]\n<class 'numpy.ndarray'>\n4\n1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), GaussianNB())\n",
    "clf.fit(trainX, trainY)\n",
    "\n",
    "print(clf.predict(testX))\n",
    "predy = clf.predict(testX)\n",
    "print(type(predy))\n",
    "\n",
    "correct = 0\n",
    "for i, ans in enumerate(testY):\n",
    "    if predy[i] == ans:\n",
    "        correct += 1\n",
    "\n",
    "print(correct)\n",
    "print(correct / float(len(testY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 0 1 1]\n[0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(testX))\n",
    "print(testY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# clf = make_pipeline(StandardScaler(), RandomForestClassifier())\n",
    "# clf.fit(trainX, trainY)\n",
    "\n",
    "# print(clf.predict(testX))\n",
    "# predy = clf.predict(testX)\n",
    "# print(type(predy))\n",
    "\n",
    "# correct = 0\n",
    "# for i, ans in enumerate(testY):\n",
    "#     if predy[i] == ans:\n",
    "#         correct += 1\n",
    "\n",
    "# print(correct)\n",
    "# print(correct / float(len(testY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0/2912 200/2912 400/2912 600/2912 boy_wonder\n",
      "800/2912 absurdity\n",
      "clasp\n",
      "aeronautical_engineering\n",
      "1000/2912 announce\n",
      "1200/2912 bishop\n",
      "1400/2912 1600/2912 1800/2912 destitution\n",
      "2000/2912 2200/2912 flit\n",
      "2400/2912 2600/2912 acolyte\n",
      "2800/2912 0/765 bring\n",
      "200/765 400/765 600/765 0/2318 200/2318 Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "400/2318 600/2318 aeronautical_engineering\n",
      "800/2318 advance\n",
      "high\n",
      "1000/2318 Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "1200/2318 1400/2318 aeronautical_engineering\n",
      "1600/2318 1800/2318 feel_for\n",
      "2000/2318 2200/2318 0/5084 200/5084 profanation\n",
      "400/5084 ambusher\n",
      "600/5084 800/5084 1000/5084 1200/5084 1400/5084 attractiveness\n",
      "1600/5084 aeronautical_engineering\n",
      "aeronautical_engineering\n",
      "1800/5084 antagonize\n",
      "2000/5084 feel\n",
      "feel_for\n",
      "2200/5084 2400/5084 alienation\n",
      "aeronautical_engineering\n",
      "2600/5084 accompany\n",
      "2800/5084 3000/5084 3200/5084 acidify\n",
      "3400/5084 high\n",
      "3600/5084 aeronautical_engineering\n",
      "3800/5084 4000/5084 bore\n",
      "aeronautical_engineering\n",
      "4200/5084 4400/5084 4600/5084 4800/5084 5000/5084 "
     ]
    }
   ],
   "source": [
    "from not_adj_transform import change_sent_not\n",
    "from clauseswitch_transform import clauseswitch\n",
    "\n",
    "#add transforms and test\n",
    "def transformaiton_pipeline(input_text):\n",
    "\n",
    "    #not transform\n",
    "    tokens = sent_tokenize(input_text)\n",
    "    num_tokens = len(tokens)\n",
    "    transformed = []\n",
    "    for i,token in enumerate(tokens):\n",
    "        after_transform = change_sent_not(token)\n",
    "        transformed.append(clauseswitch(after_transform))\n",
    "        if i % 200 == 0:\n",
    "            print(\"{}/{}\".format(i, num_tokens), end=\" \")\n",
    "    \n",
    "    \n",
    "\n",
    "    transform1_text = \"\".join(transformed)\n",
    "\n",
    "    return transform1_text\n",
    "\n",
    "\n",
    "transformed_text = []\n",
    "\n",
    "for txt in testX_text:\n",
    "    transformed_text.append(transformaiton_pipeline(txt))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/2 feature\n",
      "finish feature\n",
      "1/2 feature\n",
      "finish feature\n",
      "1/2 feature\n",
      "finish feature\n",
      "1/2 feature\n",
      "finish feature\n",
      "[1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "transformed_testX = []\n",
    "\n",
    "for txt in transformed_text:\n",
    "    transformed_testX.append(FeatureExtration(txt)) \n",
    "\n",
    "print(clf.predict(transformed_testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}