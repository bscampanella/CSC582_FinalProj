{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd048f99539accd36b27035ab6a120cdcef01e73fa85a1d9090d59c812c30161f25",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package cmudict to\n[nltk_data]     C:\\Users\\alexe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package cmudict is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\alexe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections as coll\n",
    "import math\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import style\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "\n",
    "nltk.download('cmudict')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "style.use(\"ggplot\")\n",
    "cmuDictionary = None\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def syllable_count_Manual(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "            if word.endswith(\"e\"):\n",
    "                count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# COUNTS NUMBER OF SYLLABLES\n",
    "\n",
    "def syllable_count(word):\n",
    "    global cmuDictionary\n",
    "    d = cmuDictionary\n",
    "    try:\n",
    "        syl = [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    except:\n",
    "        syl = syllable_count_Manual(word)\n",
    "    return syl\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# removing stop words plus punctuation.\n",
    "def Avg_wordLength(str):\n",
    "    str.translate(string.punctuation)\n",
    "    tokens = word_tokenize(str, language='english')\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    stop = stopwords.words('english') + st\n",
    "    words = [word for word in tokens if word not in stop]\n",
    "    return np.average([len(word) for word in words])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# returns avg number of characters in a sentence\n",
    "def Avg_SentLenghtByCh(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token) for token in tokens])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# returns avg number of words in a sentence\n",
    "def Avg_SentLenghtByWord(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token.split()) for token in tokens])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# GIVES NUMBER OF SYLLABLES PER WORD\n",
    "def Avg_Syllable_per_Word(text):\n",
    "    tokens = word_tokenize(text, language='english')\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    stop = stopwords.words('english') + st\n",
    "    words = [word for word in tokens if word not in stop]\n",
    "    syllabls = [syllable_count(word) for word in words]\n",
    "    p = (\" \".join(words))\n",
    "    return sum(syllabls) / max(1, len(words))\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# COUNTS SPECIAL CHARACTERS NORMALIZED OVER LENGTH OF CHUNK\n",
    "def CountSpecialCharacter(text):\n",
    "    st = [\"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \"/\", \"<\", \"=\", '>',\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    count = 0\n",
    "    for i in text:\n",
    "        if (i in st):\n",
    "            count = count + 1\n",
    "    return count / len(text)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def CountPuncuation(text):\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \";\", \"?\", \":\", \";\"]\n",
    "    count = 0\n",
    "    for i in text:\n",
    "        if (i in st):\n",
    "            count = count + 1\n",
    "    return float(count) / float(len(text))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# RETURNS NORMALIZED COUNT OF FUNCTIONAL WORDS FROM A Framework for\n",
    "# Authorship Identification of Online Messages: Writing-Style Features and Classification Techniques\n",
    "\n",
    "def CountFunctionalWords(text):\n",
    "    functional_words = \"\"\"a between in nor some upon\n",
    "    about both including nothing somebody us\n",
    "    above but inside of someone used\n",
    "    after by into off something via\n",
    "    all can is on such we\n",
    "    although cos it once than what\n",
    "    am do its one that whatever\n",
    "    among down latter onto the when\n",
    "    an each less opposite their where\n",
    "    and either like or them whether\n",
    "    another enough little our these which\n",
    "    any every lots outside they while\n",
    "    anybody everybody many over this who\n",
    "    anyone everyone me own those whoever\n",
    "    anything everything more past though whom\n",
    "    are few most per through whose\n",
    "    around following much plenty till will\n",
    "    as for must plus to with\n",
    "    at from my regarding toward within\n",
    "    be have near same towards without\n",
    "    because he need several under worth\n",
    "    before her neither she unless would\n",
    "    behind him no should unlike yes\n",
    "    below i nobody since until you\n",
    "    beside if none so up your\n",
    "    \"\"\"\n",
    "\n",
    "    functional_words = functional_words.split()\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    count = 0\n",
    "\n",
    "    for i in text:\n",
    "        if i in functional_words:\n",
    "            count += 1\n",
    "\n",
    "    return count / len(words)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# also returns Honore Measure R\n",
    "#s (HonoreÂ´, 1979, quoted in Holmes and Singh, 1996)\n",
    "def hapaxLegemena(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    V1 = 0\n",
    "    # dictionary comprehension . har word kay against value 0 kardi\n",
    "    freqs = {key: 0 for key in words}\n",
    "    for word in words:\n",
    "        freqs[word] += 1\n",
    "    for word in freqs:\n",
    "        if freqs[word] == 1:\n",
    "            V1 += 1\n",
    "    N = len(words)\n",
    "    V = float(len(set(words)))\n",
    "    R = 100 * math.log(N) / max(1, (1 - (V1 / V)))\n",
    "    h = V1 / N\n",
    "    return R, h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# TYPE TOKEN RATIO NO OF DIFFERENT WORDS / NO OF WORDS\n",
    "def typeTokenRatio(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "def RemoveSpecialCHs(text):\n",
    "    text = word_tokenize(text)\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "\n",
    "    words = [word for word in text if word not in st]\n",
    "    return words\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# K  10,000 * (M - N) / N**2\n",
    "# , where M  Sigma i**2 * Vi.\n",
    "def YulesCharacteristicK(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    N = len(words)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    vi = coll.Counter()\n",
    "    vi.update(freqs.values())\n",
    "    M = sum([(value * value) * vi[value] for key, value in freqs.items()])\n",
    "    K = 10000 * (M - N) / math.pow(N, 2)\n",
    "    return K\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -1*sigma(pi*lnpi)\n",
    "# Shannon and sympsons index are basically diversity indices for any community\n",
    "def ShannonEntropy(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    lenght = len(words)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    arr = np.array(list(freqs.values()))\n",
    "    distribution = 1. * arr\n",
    "    distribution /= max(1, lenght)\n",
    "    import scipy as sc\n",
    "    H = sc.stats.entropy(distribution, base=2)\n",
    "    # H = sum([(i/lenght)*math.log(i/lenght,math.e) for i in freqs.values()])\n",
    "    return H\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1 - (sigma(n(n - 1))/N(N-1)\n",
    "# N is total number of words\n",
    "# n is the number of each type of word\n",
    "def SimpsonsIndex(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    N = len(words)\n",
    "    n = sum([1.0 * i * (i - 1) for i in freqs.values()])\n",
    "    D = 1 - (n / (N * (N - 1)))\n",
    "    return D\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def FleschReadingEase(text, NoOfsentences ):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    l = float(len(words))\n",
    "    scount = 0\n",
    "    for word in words:\n",
    "        scount += syllable_count(word)\n",
    "\n",
    "    I = 206.835 - 1.015 * (l / float(NoOfsentences)) - 84.6 * (scount / float(l))\n",
    "    return I\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "def FleschCincadeGradeLevel(text, NoOfSentences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    scount = 0\n",
    "    for word in words:\n",
    "        scount += syllable_count(word)\n",
    "\n",
    "    l = len(words)\n",
    "    F = 0.39 * (l / NoOfSentences) + 11.8 * (scount / float(l)) - 15.59\n",
    "    return F\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "def dale_chall_readability_formula(text, NoOfSectences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    difficult = 0\n",
    "    adjusted = 0\n",
    "    NoOfWords = len(words)\n",
    "    with open('dale-chall.pkl', 'rb') as f:\n",
    "        fimiliarWords = pickle.load(f)\n",
    "    for word in words:\n",
    "        if word not in fimiliarWords:\n",
    "            difficult += 1\n",
    "    percent = (difficult / NoOfWords) * 100\n",
    "    if (percent > 5):\n",
    "        adjusted = 3.6365\n",
    "    D = 0.1579 * (percent) + 0.0496 * (NoOfWords / NoOfSectences) + adjusted\n",
    "    return D\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "def GunningFoxIndex(text, NoOfSentences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    NoOFWords = float(len(words))\n",
    "    complexWords = 0\n",
    "    for word in words:\n",
    "        if (syllable_count(word) > 2):\n",
    "            complexWords += 1\n",
    "\n",
    "    G = 0.4 * ((NoOFWords / NoOfSentences) + 100 * (complexWords / NoOFWords))\n",
    "    return G\n",
    "\n",
    "\n",
    "def getNumSentences(text):\n",
    "    return len([s for s in sent_tokenize(text)])\n",
    "\n",
    "\n",
    "def PrepareData(text1, text2, Winsize):\n",
    "    chunks1 = slidingWindow(text1, Winsize, Winsize)\n",
    "    chunks2 = slidingWindow(text2, Winsize, Winsize)\n",
    "    return \" \".join(str(chunk1) + str(chunk2) for chunk1, chunk2 in zip(chunks1, chunks2))\n",
    "\n",
    "def is_valid(word):\n",
    "    return not is_contraction(word) and word not in \"\"\",.'\"!?;:`~--()\\n\\n\"\"\" and word.isalpha()\n",
    "\n",
    "def get_unigram(text):\n",
    "    c = Counter([])\n",
    "    N_GRAM_COUNT = 100\n",
    "    word_list = []\n",
    "    for word in word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if is_valid(word):\n",
    "            word_list.append(word)\n",
    "    c.update(word_list)\n",
    "    f = open('uni_vocab.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "    vocab_words = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        vocab_words.append(line)\n",
    "    vocab_words = vocab_words[:N_GRAM_COUNT]    \n",
    "    vocab_words.sort()\n",
    "    feature_list = [0] * N_GRAM_COUNT\n",
    "    for word, count in c.most_common(N_GRAM_COUNT):\n",
    "        try:\n",
    "            i = vocab_words.index(word)\n",
    "            feature_list[i] = 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return feature_list\n",
    "\n",
    "def get_bigram(text):\n",
    "    c = Counter([])\n",
    "    N_GRAM_COUNT = 100\n",
    "    word_list = []\n",
    "    tokens = word_tokenize(text)\n",
    "    for i in range(1, len(tokens)):\n",
    "        prev = tokens[i - 1]\n",
    "        curr = tokens[i]\n",
    "        if is_valid(prev) and is_valid(curr):\n",
    "            word_list.append('{} {}'.format(prev, curr))\n",
    "    c.update(word_list)\n",
    "    f = open('bi_vocab.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "    vocab_words = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        vocab_words.append(line)\n",
    "    vocab_words = vocab_words[:N_GRAM_COUNT]\n",
    "    vocab_words.sort()\n",
    "    feature_list = [0] * N_GRAM_COUNT\n",
    "    for word, count in c.most_common(N_GRAM_COUNT):\n",
    "        try:\n",
    "            i = vocab_words.index(word)\n",
    "            feature_list[i] = 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return feature_list\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# returns a feature vector of text\n",
    "def FeatureExtration(text):\n",
    "    # cmu dictionary for syllables\n",
    "    global cmuDictionary\n",
    "    cmuDictionary = cmudict.dict()\n",
    "\n",
    "    chunk = text\n",
    "    feature = []\n",
    "\n",
    "    sent_num = getNumSentences(chunk)\n",
    "\n",
    "    # LEXICAL FEATURES\n",
    "    meanwl = (Avg_wordLength(chunk))\n",
    "    feature.append(meanwl)\n",
    "    \n",
    "    meansl = (Avg_SentLenghtByCh(chunk))\n",
    "    feature.append(meansl)\n",
    "    \n",
    "    mean = (Avg_SentLenghtByWord(chunk))\n",
    "    feature.append(mean)\n",
    "    \n",
    "    meanSyllable = Avg_Syllable_per_Word(chunk)\n",
    "    feature.append(meanSyllable)\n",
    "    \n",
    "    means = CountSpecialCharacter(chunk)\n",
    "    feature.append(means)\n",
    "    \n",
    "    p = CountPuncuation(chunk)\n",
    "    feature.append(p)\n",
    "    \n",
    "    f = CountFunctionalWords(text)\n",
    "    feature.append(f)\n",
    "    \n",
    "    print(\"1/2 feature\")\n",
    "    # VOCABULARY RICHNESS FEATURES\n",
    "    \n",
    "    TTratio = typeTokenRatio(chunk)\n",
    "    feature.append(TTratio)\n",
    "    \n",
    "    HonoreMeasureR, hapax = hapaxLegemena(chunk)\n",
    "    feature.append(hapax)\n",
    "    feature.append(HonoreMeasureR)\n",
    "    \n",
    "    YuleK = YulesCharacteristicK(chunk)\n",
    "    feature.append(YuleK)\n",
    "    \n",
    "    S = SimpsonsIndex(chunk)\n",
    "    feature.append(S)\n",
    "    \n",
    "    Shannon = ShannonEntropy(text)\n",
    "    feature.append(Shannon)\n",
    "\n",
    "    # READIBILTY FEATURES\n",
    "    FR = FleschReadingEase(chunk, sent_num)\n",
    "    feature.append(FR)\n",
    "\n",
    "    FC = FleschCincadeGradeLevel(chunk, sent_num)\n",
    "    feature.append(FC)\n",
    "\n",
    "    # also quite a different\n",
    "    D = dale_chall_readability_formula(chunk, sent_num)\n",
    "    feature.append(D)\n",
    "\n",
    "    # quite a difference\n",
    "    G = GunningFoxIndex(chunk, sent_num)\n",
    "    feature.append(G)\n",
    "\n",
    "    U = get_unigram(text)\n",
    "    feature.extend(U)\n",
    "\n",
    "    B = get_bigram(text)\n",
    "    feature.extend(B)\n",
    "    \n",
    "    print(\"finish feature\")\n",
    "    #vector.append(feature)\n",
    "\n",
    "    return feature\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     # You can try any text file here\n",
    "#     text = open(\"my_cd_1.txt\").read()\n",
    "\n",
    "#     vector = FeatureExtration(text)\n",
    "#     print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk import FreqDist\n",
    "from urllib import request\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "charles_dickens = [\"https://www.gutenberg.org/files/98/98-0.txt\", \"https://www.gutenberg.org/files/46/46-0.txt\", \"https://www.gutenberg.org/files/1400/1400-0.txt\", \"https://www.gutenberg.org/files/730/730-0.txt\", \"https://www.gutenberg.org/files/766/766-0.txt\", \"https://www.gutenberg.org/cache/epub/25985/pg25985.txt\", \"https://www.gutenberg.org/files/676/676-0.txt\", \"https://www.gutenberg.org/cache/epub/1023/pg1023.txt\", \"https://www.gutenberg.org/cache/epub/37121/pg37121.txt\", \"https://www.gutenberg.org/files/42232/42232-0.txt\", \"https://www.gutenberg.org/cache/epub/41894/pg41894.txt\", \"https://www.gutenberg.org/cache/epub/1415/pg1415.txt\", \"https://www.gutenberg.org/cache/epub/1394/pg1394.txt\"]\n",
    "\n",
    "marry_shelly = [\"https://www.gutenberg.org/files/84/84-0.txt\", \"https://www.gutenberg.org/files/18247/18247-0.txt\", \"https://www.gutenberg.org/cache/epub/15238/pg15238.txt\", \"https://www.gutenberg.org/cache/epub/6447/pg6447.txt\", \"https://www.gutenberg.org/files/56665/56665-0.txt\", \"https://www.gutenberg.org/files/63337/63337-0.txt\", \"https://www.gutenberg.org/files/63338/63338-0.txt\", \"https://www.gutenberg.org/files/63339/63339-0.txt\", \"https://www.gutenberg.org/files/64555/64555-0.txt\", \"https://www.gutenberg.org/files/64556/64556-0.txt\", \"https://www.gutenberg.org/files/64557/64557-0.txt\", \"https://www.gutenberg.org/cache/epub/4695/pg4695.txt\"]\n",
    "\n",
    "austin_jane = [\"https://www.gutenberg.org/files/1342/1342-0.txt\", \"https://www.gutenberg.org/files/158/158-0.txt\", \"https://www.gutenberg.org/files/161/161-0.txt\", \"https://www.gutenberg.org/cache/epub/105/pg105.txt\", \"https://www.gutenberg.org/files/121/121-0.txt\", \"https://www.gutenberg.org/files/63569/63569-0.txt\", \"https://www.gutenberg.org/files/141/141-0.txt\", \"https://www.gutenberg.org/cache/epub/946/pg946.txt\", \"https://www.gutenberg.org/cache/epub/42078/pg42078.txt\", \"https://www.gutenberg.org/files/1212/1212-0.txt\"]\n",
    "\n",
    "\n",
    "mark_twain = [\"https://www.gutenberg.org/files/142/142-0.txt\", \"https://www.gutenberg.org/files/76/76-0.txt\", \"https://www.gutenberg.org/files/76/76-0.txt\", \"https://www.gutenberg.org/files/3184/3184-0.txt\", \"https://www.gutenberg.org/files/3179/3179-0.txt\", \"https://www.gutenberg.org/cache/epub/19987/pg19987.txt\",\n",
    "\"https://www.gutenberg.org/files/3187/3187-0.txt\", \"https://www.gutenberg.org/files/86/86-0.txt\", \"https://www.gutenberg.org/files/3192/3192-0.txt\", \"https://www.gutenberg.org/files/3180/3180-0.txt\", \"https://www.gutenberg.org/files/3178/3178-0.txt\", \"https://www.gutenberg.org/files/3176/3176-0.txt\"]\n",
    "\n",
    "hg_wells = [\"https://www.gutenberg.org/files/59774/59774-0.txt\", \"https://www.gutenberg.org/files/524/524-0.txt\", \"https://www.gutenberg.org/cache/epub/19229/pg19229.txt\", \"https://www.gutenberg.org/files/59769/59769-0.txt\", \"https://www.gutenberg.org/files/1013/1013-0.txt\", \"https://www.gutenberg.org/files/456/456-0.txt\", \"https://www.gutenberg.org/cache/epub/11502/pg11502.txt\", \"https://www.gutenberg.org/cache/epub/3690/pg3690.txt\", \"https://www.gutenberg.org/files/1046/1046-0.txt\", \"https://www.gutenberg.org/files/3797/3797-0.txt\", \"https://www.gutenberg.org/files/5230/5230-0.txt\", \"https://www.gutenberg.org/cache/epub/159/pg159.txt\", \"https://www.gutenberg.org/cache/epub/39162/pg39162.txt\", \"https://www.gutenberg.org/cache/epub/11640/pg11640.txt\", \"https://www.gutenberg.org/files/1047/1047-0.txt\", \"https://www.gutenberg.org/files/60173/60173-0.txt\"]\n",
    "\n",
    "\n",
    "link_to_authors = [charles_dickens, marry_shelly, austin_jane, mark_twain, hg_wells]\n",
    "\n",
    "link_to_authors = link_to_authors[:2] #if i want to make it small for testing purposes\n",
    "\n",
    "def GetRawText(url):\n",
    "    response = request.urlopen(url)\n",
    "    raw = response.read().decode('utf8', \"ignore\")\n",
    "    #remove non book stuff\n",
    "    start_index = raw.find(\"***\")\n",
    "    end_of_line = raw.find(\"\\n\", start_index)\n",
    "    return raw[end_of_line : ]\n",
    "\n",
    "def is_contraction(text):\n",
    "    if text in \"\"\"'t''s're'll\"\"\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def create_vocabulary():\n",
    "    iters = sum([len(x) for x in link_to_authors])\n",
    "    c = Counter([])\n",
    "    i = 0\n",
    "    for author in link_to_authors:\n",
    "        print('{}/{}'.format(i, iters))\n",
    "        for link in author:\n",
    "            i += 1\n",
    "            word_list = []\n",
    "            text = GetRawText(link)\n",
    "            for word in word_tokenize(text):\n",
    "                word = word.lower()\n",
    "                if word not in stop_words and not is_contraction(word) and word not in \"\"\",.'\"!?;:`~--()\\n\\n\"\"\" and word.isalpha():\n",
    "                    word_list.append(word)\n",
    "            c.update(word_list)\n",
    "    f = open('uni_vocab.txt', 'w')    \n",
    "    for word, count in c.most_common(500):\n",
    "        f.write(word + '\\n')\n",
    "    f.close()\n",
    "\n",
    "def create_bigram_vocabulary():\n",
    "    iters = sum([len(x) for x in link_to_authors])\n",
    "    c = Counter([])\n",
    "    i = 0\n",
    "    for author in link_to_authors:\n",
    "        i += 1\n",
    "        print('{}/{}'.format(i, iters))\n",
    "        for link in author:\n",
    "            word_list = []\n",
    "            tokens = word_tokenize(GetRawText(link))\n",
    "            for i in range(1, len(tokens)):\n",
    "                prev = tokens[i - 1]\n",
    "                curr = tokens[i]\n",
    "                if is_valid(prev) and is_valid(curr):\n",
    "                    word_list.append('{} {}'.format(prev, curr))\n",
    "            c.update(word_list)\n",
    "    f = open('bi_vocab.txt', 'w')\n",
    "    for word, count in c.most_common(500):\n",
    "        f.write(word + '\\n')\n",
    "    f.close()\n",
    "\n",
    "if not os.path.exists('uni_vocab.txt'):\n",
    "    create_vocabulary()\n",
    "if not os.path.exists('bi_vocab.txt'):\n",
    "    create_bigram_vocabulary()\n",
    "\n",
    "\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "testX = []\n",
    "testY = []\n",
    "\n",
    "\n",
    "testX_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['https://www.gutenberg.org/files/98/98-0.txt', 'https://www.gutenberg.org/files/46/46-0.txt', 'https://www.gutenberg.org/files/1400/1400-0.txt', 'https://www.gutenberg.org/files/730/730-0.txt', 'https://www.gutenberg.org/files/766/766-0.txt', 'https://www.gutenberg.org/cache/epub/25985/pg25985.txt', 'https://www.gutenberg.org/files/676/676-0.txt', 'https://www.gutenberg.org/cache/epub/1023/pg1023.txt', 'https://www.gutenberg.org/cache/epub/37121/pg37121.txt', 'https://www.gutenberg.org/files/42232/42232-0.txt', 'https://www.gutenberg.org/cache/epub/41894/pg41894.txt', 'https://www.gutenberg.org/cache/epub/1415/pg1415.txt', 'https://www.gutenberg.org/cache/epub/1394/pg1394.txt']\n",
      "https://www.gutenberg.org/files/730/730-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/1023/pg1023.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/42232/42232-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/98/98-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/676/676-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/1415/pg1415.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/1400/1400-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/766/766-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/41894/pg41894.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/25985/pg25985.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/1394/pg1394.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/37121/pg37121.txt\n",
      "in test append\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/46/46-0.txt\n",
      "in test append\n",
      "1/2 feature\n",
      "finish feature\n",
      "['https://www.gutenberg.org/files/84/84-0.txt', 'https://www.gutenberg.org/files/18247/18247-0.txt', 'https://www.gutenberg.org/cache/epub/15238/pg15238.txt', 'https://www.gutenberg.org/cache/epub/6447/pg6447.txt', 'https://www.gutenberg.org/files/56665/56665-0.txt', 'https://www.gutenberg.org/files/63337/63337-0.txt', 'https://www.gutenberg.org/files/63338/63338-0.txt', 'https://www.gutenberg.org/files/63339/63339-0.txt', 'https://www.gutenberg.org/files/64555/64555-0.txt', 'https://www.gutenberg.org/files/64556/64556-0.txt', 'https://www.gutenberg.org/files/64557/64557-0.txt', 'https://www.gutenberg.org/cache/epub/4695/pg4695.txt']\n",
      "https://www.gutenberg.org/files/84/84-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/63338/63338-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/18247/18247-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/63339/63339-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/64555/64555-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/6447/pg6447.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/64556/64556-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/15238/pg15238.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/4695/pg4695.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/64557/64557-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/56665/56665-0.txt\n",
      "in test append\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/63337/63337-0.txt\n",
      "in test append\n",
      "1/2 feature\n",
      "finish feature\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#print(GetRawText(\"https://www.gutenberg.org/files/676/676-0.txt\"))\n",
    "#print(FeatureExtration(GetRawText(\"https://www.gutenberg.org/files/676/676-0.txt\")))\n",
    "\n",
    "\n",
    "\n",
    "for a, author in enumerate(link_to_authors):\n",
    "    print(author)\n",
    "    randomindex = [x for x in range(len(author))]\n",
    "    random.shuffle(randomindex) \n",
    "    author = [author[ri] for ri in randomindex]\n",
    "\n",
    "    for l, link in enumerate(author):\n",
    "        print(link)\n",
    "        if l < len(author) - 2:\n",
    "            trainY.append(a)\n",
    "            trainX.append(FeatureExtration(GetRawText(link)))\n",
    "        else:\n",
    "            print(\"in test append\")\n",
    "            testY.append(a)\n",
    "            testX_text.append(GetRawText(link))\n",
    "            testX.append(FeatureExtration(testX_text[-1]))\n",
    "\n",
    "\n",
    "randomindex = [x for x in range(len(trainX))]\n",
    "random.shuffle(randomindex)\n",
    "\n",
    "trainX = [trainX[ri] for ri in randomindex]\n",
    "trainY = [trainY[ri] for ri in randomindex]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "21\n4\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX))\n",
    "print(len(testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run this cell if you wand to save the training and testing data dimensions\n",
    "all_data = [testX, testY, trainX, trainY, testX_text]\n",
    "\n",
    "with open(\"bookdata.json\", \"w\") as f:\n",
    "    json.dump(all_data,f, indent=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#only run this if you want to load previously saved data\n",
    "with open(\"bookdata.json\", \"r\") as f:\n",
    "    all_data = json.load(f)\n",
    "#print(all_data)\n",
    "\n",
    "testX  = all_data[0]\n",
    "testY = all_data[1]\n",
    "trainX = all_data[2]\n",
    "trainY = all_data[3]\n",
    "testX_text = all_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 0 1 1]\n<class 'numpy.ndarray'>\n4\n1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), GaussianNB())\n",
    "clf.fit(trainX, trainY)\n",
    "\n",
    "print(clf.predict(testX))\n",
    "predy = clf.predict(testX)\n",
    "print(type(predy))\n",
    "\n",
    "correct = 0\n",
    "for i, ans in enumerate(testY):\n",
    "    if predy[i] == ans:\n",
    "        correct += 1\n",
    "\n",
    "print(correct)\n",
    "print(correct / float(len(testY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 0 1 1]\n[0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(testX))\n",
    "print(testY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# clf = make_pipeline(StandardScaler(), RandomForestClassifier())\n",
    "# clf.fit(trainX, trainY)\n",
    "\n",
    "# print(clf.predict(testX))\n",
    "# predy = clf.predict(testX)\n",
    "# print(type(predy))\n",
    "\n",
    "# correct = 0\n",
    "# for i, ans in enumerate(testY):\n",
    "#     if predy[i] == ans:\n",
    "#         correct += 1\n",
    "\n",
    "# print(correct)\n",
    "# print(correct / float(len(testY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0/1142 200/1142 400/1142 600/1142 800/1142 1000/1142 "
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-10454c2d27fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtestX_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mtransformed_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformaiton_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-10454c2d27fc>\u001b[0m in \u001b[0;36mtransformaiton_pipeline\u001b[1;34m(input_text)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mtransform1_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mflesch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFlesch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform1_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtransform2_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflesch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mzipf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mZipf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform2_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mtransform3_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzipf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_pct\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\GitProjects\\CSC582_FinalProj\\flesch_transform.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, decreasing, target, max_transforms)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0ms_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecreasing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[0mtransform_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msyl_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtok_tag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\GitProjects\\CSC582_FinalProj\\flesch_transform.py\u001b[0m in \u001b[0;36msort_words\u001b[1;34m(self, reverse_order)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mno_stopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m                 \u001b[1;31m# Special case for verbs, get the tense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[0mno_stopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\packages\\conda\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     21\u001b[0m         return [\n\u001b[0;32m     22\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         ]\n",
      "\u001b[1;32mE:\\packages\\conda\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\packages\\conda\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\packages\\conda\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m    207\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\packages\\conda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\packages\\conda\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\packages\\conda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file or directory: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\packages\\conda\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from not_adj_transform import change_sent_not\n",
    "from clauseswitch_transform import clauseswitch\n",
    "from synonyms import get_synonyms\n",
    "from flesch_transform import Flesch\n",
    "from zipf_transform import Zipf\n",
    "\n",
    "#add transforms and test\n",
    "def transformaiton_pipeline(input_text):\n",
    "\n",
    "    #not transform\n",
    "    tokens = sent_tokenize(input_text)\n",
    "    num_tokens = len(tokens)\n",
    "    transformed = []\n",
    "    for i,token in enumerate(tokens):\n",
    "        after_transform = change_sent_not(token)\n",
    "        transformed.append(clauseswitch(after_transform))\n",
    "        if i % 200 == 0:\n",
    "            print(\"{}/{}\".format(i, num_tokens), end=\" \")\n",
    "    \n",
    "    \n",
    "\n",
    "    transform1_text = \"\".join(transformed)\n",
    "    print('Running Flesch Transformation')\n",
    "    flesch = Flesch(transform1_text)\n",
    "    transform2_text = flesch.transform(target=70)\n",
    "    print('Running Zipf Transformation')\n",
    "    zipf = Zipf(transform2_text)\n",
    "    transform3_text = zipf.transform(word_pct=.2)\n",
    "\n",
    "    return transform3_text\n",
    "\n",
    "\n",
    "transformed_text = []\n",
    "\n",
    "for txt in testX_text:\n",
    "    transformed_text.append(transformaiton_pipeline(txt))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/2 feature\n",
      "finish feature\n",
      "1/2 feature\n",
      "finish feature\n",
      "1/2 feature\n",
      "finish feature\n",
      "1/2 feature\n",
      "finish feature\n",
      "[0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "transformed_testX = []\n",
    "\n",
    "for txt in transformed_text:\n",
    "    transformed_testX.append(FeatureExtration(txt)) \n",
    "\n",
    "print(clf.predict(transformed_testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}