{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd048f99539accd36b27035ab6a120cdcef01e73fa85a1d9090d59c812c30161f25",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package cmudict to\n[nltk_data]     C:\\Users\\alexe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package cmudict is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\alexe\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import collections as coll\n",
    "import math\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import style\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "\n",
    "nltk.download('cmudict')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "style.use(\"ggplot\")\n",
    "cmuDictionary = None\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def syllable_count_Manual(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "            if word.endswith(\"e\"):\n",
    "                count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# COUNTS NUMBER OF SYLLABLES\n",
    "\n",
    "def syllable_count(word):\n",
    "    global cmuDictionary\n",
    "    d = cmuDictionary\n",
    "    try:\n",
    "        syl = [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    except:\n",
    "        syl = syllable_count_Manual(word)\n",
    "    return syl\n",
    "\n",
    "    # ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# removing stop words plus punctuation.\n",
    "def Avg_wordLength(str):\n",
    "    str.translate(string.punctuation)\n",
    "    tokens = word_tokenize(str, language='english')\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    stop = stopwords.words('english') + st\n",
    "    words = [word for word in tokens if word not in stop]\n",
    "    return np.average([len(word) for word in words])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# returns avg number of characters in a sentence\n",
    "def Avg_SentLenghtByCh(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token) for token in tokens])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# returns avg number of words in a sentence\n",
    "def Avg_SentLenghtByWord(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token.split()) for token in tokens])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# GIVES NUMBER OF SYLLABLES PER WORD\n",
    "def Avg_Syllable_per_Word(text):\n",
    "    tokens = word_tokenize(text, language='english')\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    stop = stopwords.words('english') + st\n",
    "    words = [word for word in tokens if word not in stop]\n",
    "    syllabls = [syllable_count(word) for word in words]\n",
    "    p = (\" \".join(words))\n",
    "    return sum(syllabls) / max(1, len(words))\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# COUNTS SPECIAL CHARACTERS NORMALIZED OVER LENGTH OF CHUNK\n",
    "def CountSpecialCharacter(text):\n",
    "    st = [\"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \"/\", \"<\", \"=\", '>',\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    count = 0\n",
    "    for i in text:\n",
    "        if (i in st):\n",
    "            count = count + 1\n",
    "    return count / len(text)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def CountPuncuation(text):\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \";\", \"?\", \":\", \";\"]\n",
    "    count = 0\n",
    "    for i in text:\n",
    "        if (i in st):\n",
    "            count = count + 1\n",
    "    return float(count) / float(len(text))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# RETURNS NORMALIZED COUNT OF FUNCTIONAL WORDS FROM A Framework for\n",
    "# Authorship Identification of Online Messages: Writing-Style Features and Classification Techniques\n",
    "\n",
    "def CountFunctionalWords(text):\n",
    "    functional_words = \"\"\"a between in nor some upon\n",
    "    about both including nothing somebody us\n",
    "    above but inside of someone used\n",
    "    after by into off something via\n",
    "    all can is on such we\n",
    "    although cos it once than what\n",
    "    am do its one that whatever\n",
    "    among down latter onto the when\n",
    "    an each less opposite their where\n",
    "    and either like or them whether\n",
    "    another enough little our these which\n",
    "    any every lots outside they while\n",
    "    anybody everybody many over this who\n",
    "    anyone everyone me own those whoever\n",
    "    anything everything more past though whom\n",
    "    are few most per through whose\n",
    "    around following much plenty till will\n",
    "    as for must plus to with\n",
    "    at from my regarding toward within\n",
    "    be have near same towards without\n",
    "    because he need several under worth\n",
    "    before her neither she unless would\n",
    "    behind him no should unlike yes\n",
    "    below i nobody since until you\n",
    "    beside if none so up your\n",
    "    \"\"\"\n",
    "\n",
    "    functional_words = functional_words.split()\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    count = 0\n",
    "\n",
    "    for i in text:\n",
    "        if i in functional_words:\n",
    "            count += 1\n",
    "\n",
    "    return count / len(words)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# also returns Honore Measure R\n",
    "#s (HonoreÂ´, 1979, quoted in Holmes and Singh, 1996)\n",
    "def hapaxLegemena(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    V1 = 0\n",
    "    # dictionary comprehension . har word kay against value 0 kardi\n",
    "    freqs = {key: 0 for key in words}\n",
    "    for word in words:\n",
    "        freqs[word] += 1\n",
    "    for word in freqs:\n",
    "        if freqs[word] == 1:\n",
    "            V1 += 1\n",
    "    N = len(words)\n",
    "    V = float(len(set(words)))\n",
    "    R = 100 * math.log(N) / max(1, (1 - (V1 / V)))\n",
    "    h = V1 / N\n",
    "    return R, h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# TYPE TOKEN RATIO NO OF DIFFERENT WORDS / NO OF WORDS\n",
    "def typeTokenRatio(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "def RemoveSpecialCHs(text):\n",
    "    text = word_tokenize(text)\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "\n",
    "    words = [word for word in text if word not in st]\n",
    "    return words\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# K  10,000 * (M - N) / N**2\n",
    "# , where M  Sigma i**2 * Vi.\n",
    "def YulesCharacteristicK(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    N = len(words)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    vi = coll.Counter()\n",
    "    vi.update(freqs.values())\n",
    "    M = sum([(value * value) * vi[value] for key, value in freqs.items()])\n",
    "    K = 10000 * (M - N) / math.pow(N, 2)\n",
    "    return K\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -1*sigma(pi*lnpi)\n",
    "# Shannon and sympsons index are basically diversity indices for any community\n",
    "def ShannonEntropy(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    lenght = len(words)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    arr = np.array(list(freqs.values()))\n",
    "    distribution = 1. * arr\n",
    "    distribution /= max(1, lenght)\n",
    "    import scipy as sc\n",
    "    H = sc.stats.entropy(distribution, base=2)\n",
    "    # H = sum([(i/lenght)*math.log(i/lenght,math.e) for i in freqs.values()])\n",
    "    return H\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1 - (sigma(n(n - 1))/N(N-1)\n",
    "# N is total number of words\n",
    "# n is the number of each type of word\n",
    "def SimpsonsIndex(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    N = len(words)\n",
    "    n = sum([1.0 * i * (i - 1) for i in freqs.values()])\n",
    "    D = 1 - (n / (N * (N - 1)))\n",
    "    return D\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def FleschReadingEase(text, NoOfsentences ):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    l = float(len(words))\n",
    "    scount = 0\n",
    "    for word in words:\n",
    "        scount += syllable_count(word)\n",
    "\n",
    "    I = 206.835 - 1.015 * (l / float(NoOfsentences)) - 84.6 * (scount / float(l))\n",
    "    return I\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "def FleschCincadeGradeLevel(text, NoOfSentences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    scount = 0\n",
    "    for word in words:\n",
    "        scount += syllable_count(word)\n",
    "\n",
    "    l = len(words)\n",
    "    F = 0.39 * (l / NoOfSentences) + 11.8 * (scount / float(l)) - 15.59\n",
    "    return F\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "def dale_chall_readability_formula(text, NoOfSectences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    difficult = 0\n",
    "    adjusted = 0\n",
    "    NoOfWords = len(words)\n",
    "    with open('dale-chall.pkl', 'rb') as f:\n",
    "        fimiliarWords = pickle.load(f)\n",
    "    for word in words:\n",
    "        if word not in fimiliarWords:\n",
    "            difficult += 1\n",
    "    percent = (difficult / NoOfWords) * 100\n",
    "    if (percent > 5):\n",
    "        adjusted = 3.6365\n",
    "    D = 0.1579 * (percent) + 0.0496 * (NoOfWords / NoOfSectences) + adjusted\n",
    "    return D\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "def GunningFoxIndex(text, NoOfSentences):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    NoOFWords = float(len(words))\n",
    "    complexWords = 0\n",
    "    for word in words:\n",
    "        if (syllable_count(word) > 2):\n",
    "            complexWords += 1\n",
    "\n",
    "    G = 0.4 * ((NoOFWords / NoOfSentences) + 100 * (complexWords / NoOFWords))\n",
    "    return G\n",
    "\n",
    "\n",
    "def getNumSentences(text):\n",
    "    return len([s for s in sent_tokenize(text)])\n",
    "\n",
    "\n",
    "def PrepareData(text1, text2, Winsize):\n",
    "    chunks1 = slidingWindow(text1, Winsize, Winsize)\n",
    "    chunks2 = slidingWindow(text2, Winsize, Winsize)\n",
    "    return \" \".join(str(chunk1) + str(chunk2) for chunk1, chunk2 in zip(chunks1, chunks2))\n",
    "\n",
    "def is_valid(word):\n",
    "    return not is_contraction(word) and word not in \"\"\",.'\"!?;:`~--()\\n\\n\"\"\" and word.isalpha()\n",
    "\n",
    "def get_unigram(text):\n",
    "    c = Counter([])\n",
    "    N_GRAM_COUNT = 100\n",
    "    word_list = []\n",
    "    for word in word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if is_valid(word):\n",
    "            word_list.append(word)\n",
    "    c.update(word_list)\n",
    "    f = open('uni_vocab.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "    vocab_words = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        vocab_words.append(line)\n",
    "    vocab_words = vocab_words[:N_GRAM_COUNT]    \n",
    "    vocab_words.sort()\n",
    "    feature_list = [0] * N_GRAM_COUNT\n",
    "    for word, count in c.most_common(N_GRAM_COUNT):\n",
    "        try:\n",
    "            i = vocab_words.index(word)\n",
    "            feature_list[i] = 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return feature_list\n",
    "\n",
    "def get_bigram(text):\n",
    "    c = Counter([])\n",
    "    N_GRAM_COUNT = 100\n",
    "    word_list = []\n",
    "    tokens = word_tokenize(text)\n",
    "    for i in range(1, len(tokens)):\n",
    "        prev = tokens[i - 1]\n",
    "        curr = tokens[i]\n",
    "        if is_valid(prev) and is_valid(curr):\n",
    "            word_list.append('{} {}'.format(prev, curr))\n",
    "    c.update(word_list)\n",
    "    f = open('bi_vocab.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "    vocab_words = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        vocab_words.append(line)\n",
    "    vocab_words = vocab_words[:N_GRAM_COUNT]\n",
    "    vocab_words.sort()\n",
    "    feature_list = [0] * N_GRAM_COUNT\n",
    "    for word, count in c.most_common(N_GRAM_COUNT):\n",
    "        try:\n",
    "            i = vocab_words.index(word)\n",
    "            feature_list[i] = 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return feature_list\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# returns a feature vector of text\n",
    "def FeatureExtration(text):\n",
    "    # cmu dictionary for syllables\n",
    "    global cmuDictionary\n",
    "    cmuDictionary = cmudict.dict()\n",
    "\n",
    "    chunk = text\n",
    "    feature = []\n",
    "\n",
    "    sent_num = getNumSentences(chunk)\n",
    "\n",
    "    # LEXICAL FEATURES\n",
    "    meanwl = (Avg_wordLength(chunk))\n",
    "    feature.append(meanwl)\n",
    "    \n",
    "    meansl = (Avg_SentLenghtByCh(chunk))\n",
    "    feature.append(meansl)\n",
    "    \n",
    "    mean = (Avg_SentLenghtByWord(chunk))\n",
    "    feature.append(mean)\n",
    "    \n",
    "    meanSyllable = Avg_Syllable_per_Word(chunk)\n",
    "    feature.append(meanSyllable)\n",
    "    \n",
    "    means = CountSpecialCharacter(chunk)\n",
    "    feature.append(means)\n",
    "    \n",
    "    p = CountPuncuation(chunk)\n",
    "    feature.append(p)\n",
    "    \n",
    "    f = CountFunctionalWords(text)\n",
    "    feature.append(f)\n",
    "    \n",
    "    print(\"1/2 feature\")\n",
    "    # VOCABULARY RICHNESS FEATURES\n",
    "    \n",
    "    TTratio = typeTokenRatio(chunk)\n",
    "    feature.append(TTratio)\n",
    "    \n",
    "    HonoreMeasureR, hapax = hapaxLegemena(chunk)\n",
    "    feature.append(hapax)\n",
    "    feature.append(HonoreMeasureR)\n",
    "    \n",
    "    YuleK = YulesCharacteristicK(chunk)\n",
    "    feature.append(YuleK)\n",
    "    \n",
    "    S = SimpsonsIndex(chunk)\n",
    "    feature.append(S)\n",
    "    \n",
    "    Shannon = ShannonEntropy(text)\n",
    "    feature.append(Shannon)\n",
    "\n",
    "    # READIBILTY FEATURES\n",
    "    FR = FleschReadingEase(chunk, sent_num)\n",
    "    feature.append(FR)\n",
    "\n",
    "    FC = FleschCincadeGradeLevel(chunk, sent_num)\n",
    "    feature.append(FC)\n",
    "\n",
    "    # also quite a different\n",
    "    D = dale_chall_readability_formula(chunk, sent_num)\n",
    "    feature.append(D)\n",
    "\n",
    "    # quite a difference\n",
    "    G = GunningFoxIndex(chunk, sent_num)\n",
    "    feature.append(G)\n",
    "\n",
    "    U = get_unigram(text)\n",
    "    feature.extend(U)\n",
    "\n",
    "    B = get_bigram(text)\n",
    "    feature.extend(B)\n",
    "    \n",
    "    print(\"finish feature\")\n",
    "    #vector.append(feature)\n",
    "\n",
    "    return feature\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     # You can try any text file here\n",
    "#     text = open(\"my_cd_1.txt\").read()\n",
    "\n",
    "#     vector = FeatureExtration(text)\n",
    "#     print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk import FreqDist\n",
    "from urllib import request\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "charles_dickens = [\"https://www.gutenberg.org/files/98/98-0.txt\", \"https://www.gutenberg.org/files/46/46-0.txt\", \"https://www.gutenberg.org/files/1400/1400-0.txt\", \"https://www.gutenberg.org/files/730/730-0.txt\", \"https://www.gutenberg.org/files/766/766-0.txt\", \"https://www.gutenberg.org/cache/epub/25985/pg25985.txt\", \"https://www.gutenberg.org/files/676/676-0.txt\", \"https://www.gutenberg.org/cache/epub/1023/pg1023.txt\", \"https://www.gutenberg.org/cache/epub/37121/pg37121.txt\", \"https://www.gutenberg.org/files/42232/42232-0.txt\", \"https://www.gutenberg.org/cache/epub/41894/pg41894.txt\", \"https://www.gutenberg.org/cache/epub/1415/pg1415.txt\", \"https://www.gutenberg.org/cache/epub/1394/pg1394.txt\"]\n",
    "\n",
    "marry_shelly = [\"https://www.gutenberg.org/files/84/84-0.txt\", \"https://www.gutenberg.org/files/18247/18247-0.txt\", \"https://www.gutenberg.org/cache/epub/15238/pg15238.txt\", \"https://www.gutenberg.org/cache/epub/6447/pg6447.txt\", \"https://www.gutenberg.org/files/56665/56665-0.txt\", \"https://www.gutenberg.org/files/63337/63337-0.txt\", \"https://www.gutenberg.org/files/63338/63338-0.txt\", \"https://www.gutenberg.org/files/63339/63339-0.txt\", \"https://www.gutenberg.org/files/64555/64555-0.txt\", \"https://www.gutenberg.org/files/64556/64556-0.txt\", \"https://www.gutenberg.org/files/64557/64557-0.txt\", \"https://www.gutenberg.org/cache/epub/4695/pg4695.txt\"]\n",
    "\n",
    "austin_jane = [\"https://www.gutenberg.org/files/1342/1342-0.txt\", \"https://www.gutenberg.org/files/158/158-0.txt\", \"https://www.gutenberg.org/files/161/161-0.txt\", \"https://www.gutenberg.org/cache/epub/105/pg105.txt\", \"https://www.gutenberg.org/files/121/121-0.txt\", \"https://www.gutenberg.org/files/63569/63569-0.txt\", \"https://www.gutenberg.org/files/141/141-0.txt\", \"https://www.gutenberg.org/cache/epub/946/pg946.txt\", \"https://www.gutenberg.org/cache/epub/42078/pg42078.txt\", \"https://www.gutenberg.org/files/1212/1212-0.txt\"]\n",
    "\n",
    "\n",
    "mark_twain = [\"https://www.gutenberg.org/files/142/142-0.txt\", \"https://www.gutenberg.org/files/76/76-0.txt\", \"https://www.gutenberg.org/files/76/76-0.txt\", \"https://www.gutenberg.org/files/3184/3184-0.txt\", \"https://www.gutenberg.org/files/3179/3179-0.txt\", \"https://www.gutenberg.org/cache/epub/19987/pg19987.txt\",\n",
    "\"https://www.gutenberg.org/files/3187/3187-0.txt\", \"https://www.gutenberg.org/files/86/86-0.txt\", \"https://www.gutenberg.org/files/3192/3192-0.txt\", \"https://www.gutenberg.org/files/3180/3180-0.txt\", \"https://www.gutenberg.org/files/3178/3178-0.txt\", \"https://www.gutenberg.org/files/3176/3176-0.txt\"]\n",
    "\n",
    "hg_wells = [\"https://www.gutenberg.org/files/59774/59774-0.txt\", \"https://www.gutenberg.org/files/524/524-0.txt\", \"https://www.gutenberg.org/cache/epub/19229/pg19229.txt\", \"https://www.gutenberg.org/files/59769/59769-0.txt\", \"https://www.gutenberg.org/files/1013/1013-0.txt\", \"https://www.gutenberg.org/files/456/456-0.txt\", \"https://www.gutenberg.org/cache/epub/11502/pg11502.txt\", \"https://www.gutenberg.org/cache/epub/3690/pg3690.txt\", \"https://www.gutenberg.org/files/1046/1046-0.txt\", \"https://www.gutenberg.org/files/3797/3797-0.txt\", \"https://www.gutenberg.org/files/5230/5230-0.txt\", \"https://www.gutenberg.org/cache/epub/159/pg159.txt\", \"https://www.gutenberg.org/cache/epub/39162/pg39162.txt\", \"https://www.gutenberg.org/cache/epub/11640/pg11640.txt\", \"https://www.gutenberg.org/files/1047/1047-0.txt\", \"https://www.gutenberg.org/files/60173/60173-0.txt\"]\n",
    "\n",
    "\n",
    "link_to_authors = [charles_dickens, marry_shelly, austin_jane, mark_twain, hg_wells]\n",
    "\n",
    "# link_to_authors = link_to_authors[:2] #if i want to make it small for testing purposes\n",
    "\n",
    "def GetRawText(url):\n",
    "    response = request.urlopen(url)\n",
    "    raw = response.read().decode('utf8', \"ignore\")\n",
    "    #remove non book stuff\n",
    "    start_index = raw.find(\"***\")\n",
    "    end_of_line = raw.find(\"\\n\", start_index)\n",
    "    return raw[end_of_line : ]\n",
    "\n",
    "def is_contraction(text):\n",
    "    if text in \"\"\"'t''s're'll\"\"\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def create_vocabulary():\n",
    "    iters = sum([len(x) for x in link_to_authors])\n",
    "    c = Counter([])\n",
    "    i = 0\n",
    "    for author in link_to_authors:\n",
    "        print('{}/{}'.format(i, iters))\n",
    "        for link in author:\n",
    "            i += 1\n",
    "            word_list = []\n",
    "            text = GetRawText(link)\n",
    "            for word in word_tokenize(text):\n",
    "                word = word.lower()\n",
    "                if word not in stop_words and not is_contraction(word) and word not in \"\"\",.'\"!?;:`~--()\\n\\n\"\"\" and word.isalpha():\n",
    "                    word_list.append(word)\n",
    "            c.update(word_list)\n",
    "    f = open('uni_vocab.txt', 'w')    \n",
    "    for word, count in c.most_common(500):\n",
    "        f.write(word + '\\n')\n",
    "    f.close()\n",
    "\n",
    "def create_bigram_vocabulary():\n",
    "    iters = sum([len(x) for x in link_to_authors])\n",
    "    c = Counter([])\n",
    "    i = 0\n",
    "    for author in link_to_authors:\n",
    "        i += 1\n",
    "        print('{}/{}'.format(i, iters))\n",
    "        for link in author:\n",
    "            word_list = []\n",
    "            tokens = word_tokenize(GetRawText(link))\n",
    "            for i in range(1, len(tokens)):\n",
    "                prev = tokens[i - 1]\n",
    "                curr = tokens[i]\n",
    "                if is_valid(prev) and is_valid(curr):\n",
    "                    word_list.append('{} {}'.format(prev, curr))\n",
    "            c.update(word_list)\n",
    "    f = open('bi_vocab.txt', 'w')\n",
    "    for word, count in c.most_common(500):\n",
    "        f.write(word + '\\n')\n",
    "    f.close()\n",
    "\n",
    "if not os.path.exists('uni_vocab.txt'):\n",
    "    create_vocabulary()\n",
    "if not os.path.exists('bi_vocab.txt'):\n",
    "    create_bigram_vocabulary()\n",
    "\n",
    "\n",
    "trainX = []\n",
    "trainY = []\n",
    "\n",
    "testX = []\n",
    "testY = []\n",
    "\n",
    "\n",
    "testX_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['https://www.gutenberg.org/files/98/98-0.txt', 'https://www.gutenberg.org/files/46/46-0.txt', 'https://www.gutenberg.org/files/1400/1400-0.txt', 'https://www.gutenberg.org/files/730/730-0.txt', 'https://www.gutenberg.org/files/766/766-0.txt', 'https://www.gutenberg.org/cache/epub/25985/pg25985.txt', 'https://www.gutenberg.org/files/676/676-0.txt', 'https://www.gutenberg.org/cache/epub/1023/pg1023.txt', 'https://www.gutenberg.org/cache/epub/37121/pg37121.txt', 'https://www.gutenberg.org/files/42232/42232-0.txt', 'https://www.gutenberg.org/cache/epub/41894/pg41894.txt', 'https://www.gutenberg.org/cache/epub/1415/pg1415.txt', 'https://www.gutenberg.org/cache/epub/1394/pg1394.txt']\n",
      "https://www.gutenberg.org/files/730/730-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/1023/pg1023.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/42232/42232-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/98/98-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/676/676-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/1415/pg1415.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/1400/1400-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/766/766-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/41894/pg41894.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/25985/pg25985.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/1394/pg1394.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/37121/pg37121.txt\n",
      "in test append\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/46/46-0.txt\n",
      "in test append\n",
      "1/2 feature\n",
      "finish feature\n",
      "['https://www.gutenberg.org/files/84/84-0.txt', 'https://www.gutenberg.org/files/18247/18247-0.txt', 'https://www.gutenberg.org/cache/epub/15238/pg15238.txt', 'https://www.gutenberg.org/cache/epub/6447/pg6447.txt', 'https://www.gutenberg.org/files/56665/56665-0.txt', 'https://www.gutenberg.org/files/63337/63337-0.txt', 'https://www.gutenberg.org/files/63338/63338-0.txt', 'https://www.gutenberg.org/files/63339/63339-0.txt', 'https://www.gutenberg.org/files/64555/64555-0.txt', 'https://www.gutenberg.org/files/64556/64556-0.txt', 'https://www.gutenberg.org/files/64557/64557-0.txt', 'https://www.gutenberg.org/cache/epub/4695/pg4695.txt']\n",
      "https://www.gutenberg.org/files/84/84-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/63338/63338-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/18247/18247-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/63339/63339-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/64555/64555-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/6447/pg6447.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/64556/64556-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/15238/pg15238.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/cache/epub/4695/pg4695.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/64557/64557-0.txt\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/56665/56665-0.txt\n",
      "in test append\n",
      "1/2 feature\n",
      "finish feature\n",
      "https://www.gutenberg.org/files/63337/63337-0.txt\n",
      "in test append\n",
      "1/2 feature\n",
      "finish feature\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#print(GetRawText(\"https://www.gutenberg.org/files/676/676-0.txt\"))\n",
    "#print(FeatureExtration(GetRawText(\"https://www.gutenberg.org/files/676/676-0.txt\")))\n",
    "\n",
    "\n",
    "\n",
    "for a, author in enumerate(link_to_authors):\n",
    "    print(author)\n",
    "    randomindex = [x for x in range(len(author))]\n",
    "    random.shuffle(randomindex) \n",
    "    author = [author[ri] for ri in randomindex]\n",
    "\n",
    "    for l, link in enumerate(author):\n",
    "        print(link)\n",
    "        if l < len(author) - 2:\n",
    "            trainY.append(a)\n",
    "            trainX.append(FeatureExtration(GetRawText(link)))\n",
    "        else:\n",
    "            print(\"in test append\")\n",
    "            testY.append(a)\n",
    "            testX_text.append(GetRawText(link))\n",
    "            testX.append(FeatureExtration(testX_text[-1]))\n",
    "\n",
    "\n",
    "randomindex = [x for x in range(len(trainX))]\n",
    "random.shuffle(randomindex)\n",
    "\n",
    "trainX = [trainX[ri] for ri in randomindex]\n",
    "trainY = [trainY[ri] for ri in randomindex]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "21\n4\n"
     ]
    }
   ],
   "source": [
    "print(len(trainX))\n",
    "print(len(testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run this cell if you wand to save the training and testing data dimensions\n",
    "all_data = [testX, testY, trainX, trainY, testX_text]\n",
    "\n",
    "with open(\"bookdata.json\", \"w\") as f:\n",
    "    json.dump(all_data,f, indent=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#only run this if you want to load previously saved data\n",
    "with open(\"bookdata.json\", \"r\") as f:\n",
    "    all_data = json.load(f)\n",
    "#print(all_data)\n",
    "\n",
    "testX  = all_data[0]\n",
    "testY = all_data[1]\n",
    "trainX = all_data[2]\n",
    "trainY = all_data[3]\n",
    "testX_text = all_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 0 1 1]\n<class 'numpy.ndarray'>\n4\n1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), GaussianNB())\n",
    "clf.fit(trainX, trainY)\n",
    "\n",
    "print(clf.predict(testX))\n",
    "predy = clf.predict(testX)\n",
    "print(type(predy))\n",
    "\n",
    "correct = 0\n",
    "for i, ans in enumerate(testY):\n",
    "    if predy[i] == ans:\n",
    "        correct += 1\n",
    "\n",
    "print(correct)\n",
    "print(correct / float(len(testY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 0 1 1]\n[0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(testX))\n",
    "print(testY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# clf = make_pipeline(StandardScaler(), RandomForestClassifier())\n",
    "# clf.fit(trainX, trainY)\n",
    "\n",
    "# print(clf.predict(testX))\n",
    "# predy = clf.predict(testX)\n",
    "# print(type(predy))\n",
    "\n",
    "# correct = 0\n",
    "# for i, ans in enumerate(testY):\n",
    "#     if predy[i] == ans:\n",
    "#         correct += 1\n",
    "\n",
    "# print(correct)\n",
    "# print(correct / float(len(testY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ll raise your wage, and\n",
      "strive to help your struggling house, and we will discuss\n",
      "your affairs this very afternoon, over a Christmas bowl of\n",
      "smoking bishop, Bob!Before you dot another i, Bob Cratchit! , make up the fires, and buy another coal-scuttle \"Scrooge was not worse, it was better than his word . He did it all, and\n",
      "boundlessly more; and to Tiny Tim, who did NOT die, he was\n",
      "a 2nd father.He went as good a friend, as good a\n",
      "master, and as good a man, as the good old city knew, or\n",
      "any other good old city, town, or borough, in the good old\n",
      "world.Some people laughed to see the change in him , \n",
      " but he let them laugh , and little heeded them ; for he was \n",
      " wise enough to know that nothing ever happened on this \n",
      " globe , for good , at which some people did not have their fill \n",
      " of laughter in the first ; and knowing that such as these \n",
      " would be not sighted, their be blind anyway , he thought it quite as well that they \n",
      " should purse up their eyes in grins , as have the malady in \n",
      " less attractive forms . His own heart laughed: and that was\n",
      "quite enough for him.If any man live possessed the knowledge , he had no further intercourse with Spirits, but dwelt upon the Total Abstinence Principle, ever later; and it was always said of him, that he knew how to keep Christmas well, .May that\n",
      "be truly said of us, and all of us!And so, as Tiny Tim\n",
      "observed, God bless Us, Every One!End of the Project Gutenberg EBook of A Christmas Carol, by Charles Dickens\n",
      "\n",
      "*** END OF THIS PROJECT GUTENBERG EBOOK A CHRISTMAS CAROL ***\n",
      "\n",
      "***** This file should be named 46-8.txt or 46-8.zip *****\n",
      "This and all associated files of various formats will be found in:\n",
      "        https://www.gutenberg.org/4/46/\n",
      "\n",
      "produced by Jose Menendez\n",
      "\n",
      "updated editions will replace the old one--the old editions\n",
      "will be renamed.making the works from public sphere print editions means that no\n",
      "one owns a United States copyright in these works, so the Foundation\n",
      "(and you!)can copy and deal it in the United States without\n",
      "permission and without paying copyright royalties.Special rules,\n",
      "set forth in the General Terms of Use part of this license, use to\n",
      "copying and distributing Project Gutenberg-tm electronic works to\n",
      "protect the PROJECT GUTENBERG-tm concept and hallmark.Unless you receive specific permission , task Gutenberg is a registered hallmark, and may not be used if you charge for the eBooks, .If you \n",
      " do not charge anything for copies of this eBook , complying with the \n",
      " rules is not hard, anything is very easy . You may use this eBook for nearly any purpose\n",
      "such as creation of derivative works, reports, doances and\n",
      "research.They may be modified and printed and given away--you may do\n",
      "practically ANYTHING with public sphere eBooks.Redistribution is\n",
      "subject to the hallmark license, specially commercial\n",
      "redistribution.*** START: FULL LICENSE ***\n",
      "\n",
      "THE FULL PROJECT GUTENBERG LICENSE\n",
      "PLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK\n",
      "\n",
      "To protect the Project Gutenberg-tm mission of promoting the free\n",
      "distribution of electronic works, by using or distributing this work\n",
      "(or any other work associated in any way with the phrase \"Project\n",
      "Gutenberg\"), you agree to comply with all the terms of the Full Project\n",
      "Gutenberg-tm License (available with this file or online at\n",
      "https://gutenberg.org/license).Section 1.General Terms of Use and Redistributing Project Gutenberg-tm\n",
      "electronic works\n",
      "\n",
      "1.A.By reading or using any part of this Project Gutenberg-tm\n",
      "electronic work, you bespeak that you have read, understand, agree to\n",
      "and accept all the terms of this license and rational hold\n",
      "(hallmark/copyright) agreement.If you do not agree to bide by all\n",
      "the terms of this agreement, you must cease using and return or destroy\n",
      "all copies of Project Gutenberg-tm electronic works in your possession.If you paid a fee for obtaining a copy of or entree to a Project\n",
      "Gutenberg-tm electronic work and you do not agree to be bound by the\n",
      "terms of this agreement, you may obtain a refund from the soul or\n",
      "entity to whom you paid the fee as set forth in paragraph 1.E.8.1.B.\"Project Gutenberg\" is a registered hallmark.It may only be\n",
      "used on or associated in any way with an electronic work by people who\n",
      "agree to be bound by the terms of this agreement.There are a few\n",
      "things that you can do with most Project Gutenberg-tm electronic works\n",
      "even without complying with the full terms of this agreement.See\n",
      "paragraph 1.C below.If you follow the terms of this agreement and help preserve free future entree to Project Gutenberg-tm electronic works , There are a lot of things you can do with Project Gutenberg-tm electronic works .See paragraph 1.E below.1.C.The Project Gutenberg Literary Archive Foundation (\"the Foundation\"\n",
      "or PGLAF), owns a digest copyright in the collection of Project\n",
      "Gutenberg-tm electronic works.most all the single works in the\n",
      "collection are in the public sphere in the United States.If an\n",
      "single work is in the public sphere in the United States and you are\n",
      "located in the United States, we do not claim a right to prevent you from\n",
      "copying, distributing, performing, showing or making derivative\n",
      "works based on the work as long as all mentions to Project Gutenberg\n",
      "are taked.Of course, we hope that you will support the Project\n",
      "Gutenberg-tm mission of promoting free entree to electronic works by\n",
      "freely sharing Project Gutenberg-tm works in compliance with the terms of\n",
      "this agreement for keeping the Project Gutenberg-tm name associated with\n",
      "the work.You can easy comply with the terms of this agreement by\n",
      "keeping this work in the same format with its attached full Project\n",
      "Gutenberg-tm License when you share it without charge with others.1.D.The copyright laws of the place where you are located also govern\n",
      "what you can do with this work.Copyright laws in most states are in\n",
      "a constant state of change.If you are outside the United States, check\n",
      "the laws of your state in add to the terms of this agreement\n",
      "before downloading, copying, displaying, performing, distributing or\n",
      "making derivative works based on this work or any other Project\n",
      "Gutenberg-tm work.The Foundation makes no representations referring\n",
      "the copyright status of any work in any state outside the United\n",
      "States.1.E.Unless you have removed all mentions to Project Gutenberg:\n",
      "\n",
      "1.E.1.The following sentence, with active links to, or other immediate\n",
      "entree to, the full Project Gutenberg-tm License must appear prominently\n",
      "whenever any copy of a Project Gutenberg-tm work (any work on which the\n",
      "phrase \"Project Gutenberg\" looks, or with which the phrase \"Project\n",
      "Gutenberg\" is associated) is entreeed, displayed, performed, seen,\n",
      "copied or deald:\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "most no restrictions whatsoever.You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n",
      "with this eBook or online at www.gutenberg.org\n",
      "\n",
      "1.E.2.If an single Project Gutenberg-tm electronic work is deduced\n",
      "from the public sphere (does not contain a notice bespeaking that it is\n",
      "posted with permission of the copyright holder), the work can be copied\n",
      "and deald to anyone in the United States without paying any fees\n",
      "or charges.If you are redistributing or supplying entree to a work\n",
      "with the phrase \"Project Gutenberg\" associated with or looking on the\n",
      "work, you must comply either with the demands of paragraphs 1.E.1\n",
      "through 1.E.7 or obtain permission for the use of the work and the\n",
      "Project Gutenberg-tm hallmark as set forth in paragraphs 1.E.8 or\n",
      "1.E.9.1.E.3.If an single Project Gutenberg-tm electronic work is posted\n",
      "with the permission of the copyright holder, your use and distribution\n",
      "must comply with both paragraphs 1.E.1 through 1.E.7 and any extra\n",
      "terms enforced by the copyright holder.extra terms will be linked\n",
      "to the Project Gutenberg-tm License for all works posted with the\n",
      "permission of the copyright holder found at the beginning of this work.1.E.4.Do not unlink or detach or remove the full Project Gutenberg-tm\n",
      "License terms from this work, or any files containing a part of this\n",
      "work or any other work associated with Project Gutenberg-tm.1.E.5.Do not copy, display, perform, deal or redeal this\n",
      "electronic work, or any part of this electronic work, without\n",
      "prominently displaying the sentence set forth in paragraph 1.E.1 with\n",
      "active links or immediate entree to the full terms of the Project\n",
      "Gutenberg-tm License.1.E.6.You may convert to and distribute this work in any binary,\n",
      "compressed, tagged up, nonproprietary or proprietary form, including any\n",
      "word processing or hypertext form.If you provide entree to or distribute copies of a Project Gutenberg-tm work in a format other than \"Plain Vanilla ASCII\" or other format used in the official version posted on the official Project Gutenberg-tm web site (www.gutenberg.org), you must, at no extra cost, fee or expense to the user, provide a copy, a means of exporting a copy, or a means of obtaining a copy upon request, of the work in its original \"Plain Vanilla ASCII\" or other form , however, .Any alternate format must include the full Project Gutenberg-tm\n",
      "License as specified in paragraph 1.E.1.1.E.7.Do not charge a fee for entree to, seeing, displaying,\n",
      "performing, copying or distributing any Project Gutenberg-tm works\n",
      "unless you comply with paragraph 1.E.8 or 1.E.9.1.E.8.You may charge a sensible fee for copies of or supplying\n",
      "entree to or distributing Project Gutenberg-tm electronic works supplied\n",
      "that\n",
      "\n",
      "- You pay a royalty fee of 20% of the gross nets you derive from\n",
      "     the use of Project Gutenberg-tm works cipherd using the method\n",
      "     you already use to calculate your applicable taxes.The fee is\n",
      "     owed to the owner of the Project Gutenberg-tm hallmark, but he\n",
      "     has agreed to donate royalties under this paragraph to the\n",
      "     Project Gutenberg Literary Archive Foundation.Royalty payments\n",
      "     must be paid within 60 days following each date on which you\n",
      "     prepare (or are de jure asked to prepare) your periodic tax\n",
      "     returns.Royalty payments should be clearly tagged as such and\n",
      "     sent to the Project Gutenberg Literary Archive Foundation at the\n",
      "     address specified in Section 4, \"info about donations to\n",
      "     the Project Gutenberg Literary Archive Foundation.\"- You provide a full refund of any money paid by a user who advises\n",
      "     you in writing (or by e-mail) within 30 days of receipt that s/he\n",
      "     does not agree to the terms of the full Project Gutenberg-tm\n",
      "     License.You must ask such a user to return or\n",
      "     destroy all copies of the works possessed in a physical medium\n",
      "     and stop all use of and all access to other copies of\n",
      "     Project Gutenberg-tm works.- You provide, in accord with paragraph 1.F.3, a full refund of any\n",
      "     money paid for a work or a replace copy, if a defect in the\n",
      "     electronic work is detected and reported to you within 90 days\n",
      "     of receipt of the work.- You comply with all other terms of this agreement for free\n",
      "     distribution of Project Gutenberg-tm works.1.E.9.If you wish to charge a fee or distribute a Project Gutenberg-tm\n",
      "electronic work or group of works on different terms than are set\n",
      "forth in this agreement, you must obtain permission in writing from\n",
      "both the Project Gutenberg Literary Archive Foundation and Michael\n",
      "Hart, the owner of the Project Gutenberg-tm hallmark.Contact the\n",
      "Foundation as set forth in Section 3 below.1.F.1.F.1.Project Gutenberg volunteers and employees use considerable\n",
      "try to place, do copyright research on, transcribe and proofread\n",
      "public sphere works in making the Project Gutenberg-tm\n",
      "collection.Despite these tries, Project Gutenberg-tm electronic\n",
      "works, and the medium on which they may be stored, may contain\n",
      "\"Defects,\" such as, but not restricted to, incomplete, inaccurate or\n",
      "corrupt data, transcription errors, a copyright or other rational\n",
      "hold violation, a faulty or damaged disk or other medium, a\n",
      "computer virus, or computer codes that damage or cannot be read by\n",
      "your equipment.1.F.2.LIMITED guarantee, DISCLAIMER OF DAMAGES - Except for the \"Right\n",
      "of Replacement or Refund\" described in paragraph 1.F.3, the Project\n",
      "Gutenberg Literary Archive Foundation, the owner of the Project\n",
      "Gutenberg-tm hallmark, and any other party distributing a Project\n",
      "Gutenberg-tm electronic work under this agreement, disclaim all\n",
      "liability to you for amends, costs and expenses, including legal\n",
      "fees.YOU AGREE THAT YOU HAVE NO redress FOR NEGLIGENCE, STRICT\n",
      "LIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE\n",
      "PROVIDED IN PARAGRAPH F3.YOU AGREE THAT THE FOUNDATION, THE\n",
      "TRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE\n",
      "LIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR\n",
      "INCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH\n",
      "DAMAGE.1.F.3.LIMITED RIGHT OF replace OR REFUND - If you detect a\n",
      "defect in this electronic work within 90 days of receiving it, you can\n",
      "receive a refund of the money (if any) you paid for it by sending a\n",
      "written account to the soul you received the work from.If you\n",
      "received the work on a physical medium, you must return the medium with\n",
      "your written account.The soul or entity that supplied you with\n",
      "the faulty work may elect to provide a replace copy in lieu of a\n",
      "refund.If you received the work electronically, the person or entity\n",
      "supplying it to you may choose to give you a 2nd chance to\n",
      "receive the work electronically in lieu of a refund.If the 2nd copy \n",
      " is not faulty, 2nd is also faulty , you may demand a refund in writing without further \n",
      " chances to fix the job . 1.F.4.Except for the limited right of replace or refund set forth\n",
      "in paragraph 1.F.3, this work is supplied to you 'AS-IS' WITH NO OTHER\n",
      "guarantees OF ANY KIND, EXPRESS OR IMPLIED, including BUT NOT LIMITED TO\n",
      "WARRANTIES OF MERCHANTIBILITY OR FITNESS FOR ANY PURPOSE.1.F.5.Some states do not allow disclaimers of certain implied\n",
      "guarantees or the exclusion or restriction of certain types of amends.If any disclaimer or restriction set forth in this agreement violates the\n",
      "law of the state applicable to this agreement, the agreement shall be\n",
      "interpreted to make the maximum disclaimer or restriction permitted by\n",
      "the applicable state law.The invalidness or unenforceability of any\n",
      "provision of this agreement shall not void the staying provisions.1.F.6.INDEMNITY - You agree to indemnify and hold the Foundation, the\n",
      "hallmark owner, any agent or employee of the Foundation, anyone\n",
      "supplying copies of Project Gutenberg-tm electronic works in accord\n",
      "with this agreement, and any volunteers associated with the production,\n",
      "promotion and distribution of Project Gutenberg-tm electronic works,\n",
      "harmless from all liability, costs and expenses, including legal fees,\n",
      "that rise straight or indirectly from any of the following which you do\n",
      "or cause to hap: (a) distribution of this or any Project Gutenberg-tm\n",
      "work, (b) change, alteration, or adds or deletions to any\n",
      "Project Gutenberg-tm work, and (c) any Defect you cause.Section  2.info about the Mission of Project Gutenberg - tm \n",
      "\n",
      " Project Gutenberg - tm is not antonymous, the Mission of Project is synonymous with the free distribution of \n",
      " electronic works in formats clear by the wide mixture of computers \n",
      " including obsolete , old , middle - aged and new computers . It is\n",
      "because of the tries of 100s of volunteers and donations from\n",
      "people in all walks of life.Volunteers and fiscal support to provide volunteers with the \n",
      " aid they need , is not uncritical, they is critical to reaching Project Gutenberg - tm 's \n",
      " goals and guaranteeing that the Project Gutenberg - tm collection will \n",
      " stay freely available for coevals to come . In 2001, the Project\n",
      "Gutenberg Literary Archive Foundation was made to provide a secure\n",
      "and lasting future for Project Gutenberg-tm and future coevals.To learn more about the Project Gutenberg Literary Archive Foundation\n",
      "and how your tries and donations can help, see Sections 3 and 4\n",
      "and the Foundation web page at https://www.pglaf.org.Section 3.Information about the Project Gutenberg Literary Archive\n",
      "Foundation\n",
      "\n",
      "The Project Gutenberg Literary Archive Foundation is a non profit\n",
      "501(c)(3) educational corp formed under the laws of the\n",
      "state of Mississippi and granted tax exempt status by the Internal\n",
      "Revenue Service.The Foundation's EIN or federal tax designation\n",
      "number is 64-6221541.Its 501(c)(3) letter is posted at\n",
      "https://pglaf.org/fundraising.Contributions to the Project Gutenberg\n",
      "Literary Archive Foundation are tax deductible to the full extent\n",
      "permitted by U.S. federal laws and your state's laws.The Foundation's chief office is located at 4557 Melan Dr. S.\n",
      "Fairbanks, AK, 99712., but its volunteers and employees are dispersed\n",
      "throughout legion locations.Its business office is located at\n",
      "809 North 1500 West, Salt Lake City, UT 84116, (801) 596-1887, email\n",
      "business@pglaf.org.Email contact links and up to date contact\n",
      "info can be found at the Foundation's web site and official\n",
      "page at https://pglaf.org\n",
      "\n",
      "For extra contact info:\n",
      "     Dr. Gregory B. Newby\n",
      "     Chief Executive and Director\n",
      "     gbnewby@pglaf.org\n",
      "\n",
      "\n",
      "Section 4.Information about Donations to the Project Gutenberg\n",
      "Literary Archive Foundation\n",
      "\n",
      "Project Gutenberg-tm depends upon and cannot last without wide\n",
      "spread public support and donations to carry out its mission of\n",
      "increasing the number of public sphere and licensed works that can be\n",
      "freely distributed in machine clear form accessible by the wide\n",
      "array of equipment including outdated equipment.Many small donations \n",
      " ( $ 1 to $ 5,000 ) are not unimportant, $ 1 to $ 5,000 are peculiarly important to keeping tax exempt \n",
      " status with the IRS . The Foundation is committed to complying with the laws regulating\n",
      "charities and charitable donations in all 50 states of the United\n",
      "States.Compliance demands are not uniform and it takes a\n",
      "considerable try, much paperwork and many fees to meet and keep up\n",
      "with these demands.We do not beg donations in locations\n",
      "where we have not received written check of compliance.To\n",
      "SEND DONATIONS or find the status of compliance for any\n",
      "peculiar state visit https://pglaf.org\n",
      "\n",
      "While we cannot and do not beg parts from states where we\n",
      "have not met the solicitation demands, we know of no prohibition\n",
      "against accepting unasked donations from donors in such states who\n",
      "approach us with offers to donate.International donations are gratefully accepted, but we cannot make\n",
      "any statements referring tax treatment of donations received from\n",
      "outside the United States.U.S. laws alone swamp our small staff.Please check the Project Gutenberg Web pages for current donation\n",
      "methods and addresses.Donations are accepted in a number of other\n",
      "ways including including checks, online payments and credit card\n",
      "donations.To donate, please visit: https://pglaf.org/donate\n",
      "\n",
      "\n",
      "Section 5.General Information About Project Gutenberg-tm electronic\n",
      "works.Professor Michael S. Hart was the conceiver of the Project Gutenberg-tm\n",
      "concept of a library of electronic works that could be freely shared\n",
      "with anyone.For thirty years, he produced and distributed Project\n",
      "Gutenberg-tm eBooks with only a loose web of volunteer support.Project Gutenberg-tm eBooks are often made from several printed\n",
      "editions, all of which are confirmed as Public Domain in the U.S.\n",
      "unless a copyright notice is included.Thus, we do not needfully\n",
      "keep eBooks in compliance with any peculiar paper edition.Most people start at our Web site which has the main PG search facility:\n",
      "\n",
      "     https://www.gutenberg.org\n",
      "\n",
      "This Web site includes info about Project Gutenberg-tm,\n",
      "including how to make donations to the Project Gutenberg Literary\n",
      "Archive Foundation, how to help produce our new eBooks, and how to\n",
      "subscribe to our email newssheet to hear about new eBooks.\n",
      "Running Zipf Transformation\n",
      "0/5084 200/5084 400/5084 600/5084 800/5084 1000/5084 1200/5084 1400/5084 1600/5084 1800/5084 2000/5084 2200/5084 2400/5084 2600/5084 2800/5084 3000/5084 3200/5084 3400/5084 3600/5084 3800/5084 4000/5084 4200/5084 4400/5084 4600/5084 4800/5084 5000/5084 Running Flesch Transformation\n"
     ]
    }
   ],
   "source": [
    "from not_adj_transform import change_sent_not\n",
    "from clauseswitch_transform import clauseswitch\n",
    "from synonyms import get_synonyms\n",
    "from flesch_transform import Flesch\n",
    "from zipf_transform import Zipf\n",
    "\n",
    "#add transforms and test\n",
    "def transformaiton_pipeline(input_text):\n",
    "\n",
    "    #not transform\n",
    "    tokens = sent_tokenize(input_text)\n",
    "    num_tokens = len(tokens)\n",
    "    transformed = []\n",
    "    for i,token in enumerate(tokens):\n",
    "        after_transform = change_sent_not(token)\n",
    "        transformed.append(clauseswitch(after_transform))\n",
    "        if i % 200 == 0:\n",
    "            print(\"{}/{}\".format(i, num_tokens), end=\" \")\n",
    "    \n",
    "    \n",
    "\n",
    "    transform1_text = \"\".join(transformed)\n",
    "    print('Running Flesch Transformation')\n",
    "    flesch = Flesch(transform1_text)\n",
    "    transform2_text = flesch.transform(target=70)\n",
    "    print('Running Zipf Transformation')\n",
    "    zipf = Zipf(transform2_text)\n",
    "    transform3_text = zipf.transform(word_pct=.2)\n",
    "\n",
    "    return transform3_text\n",
    "\n",
    "\n",
    "transformed_text = []\n",
    "\n",
    "for txt in testX_text:\n",
    "    transformed_text.append(transformaiton_pipeline(txt))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_testX = []\n",
    "\n",
    "for txt in transformed_text:\n",
    "    transformed_testX.append(FeatureExtration(txt)) \n",
    "\n",
    "print(clf.predict(transformed_testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}